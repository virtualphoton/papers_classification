{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95867784-efdc-4451-841d-59d988c72602",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxonomy = \"\"\"\n",
    "cs.AI (Artificial Intelligence)\n",
    "\n",
    "Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.\n",
    "cs.AR (Hardware Architecture)\n",
    "\n",
    "Covers systems organization and hardware architecture. Roughly includes material in ACM Subject Classes C.0, C.1, and C.5.\n",
    "cs.CC (Computational Complexity)\n",
    "\n",
    "Covers models of computation, complexity classes, structural complexity, complexity tradeoffs, upper and lower bounds. Roughly includes material in ACM Subject Classes F.1 (computation by abstract devices), F.2.3 (tradeoffs among complexity measures), and F.4.3 (formal languages), although some material in formal languages may be more appropriate for Logic in Computer Science. Some material in F.2.1 and F.2.2, may also be appropriate here, but is more likely to have Data Structures and Algorithms as the primary subject area.\n",
    "cs.CE (Computational Engineering, Finance, and Science)\n",
    "\n",
    "Covers applications of computer science to the mathematical modeling of complex systems in the fields of science, engineering, and finance. Papers here are interdisciplinary and applications-oriented, focusing on techniques and tools that enable challenging computational simulations to be performed, for which the use of supercomputers or distributed computing platforms is often required. Includes material in ACM Subject Classes J.2, J.3, and J.4 (economics).\n",
    "cs.CG (Computational Geometry)\n",
    "\n",
    "Roughly includes material in ACM Subject Classes I.3.5 and F.2.2.\n",
    "cs.CL (Computation and Language)\n",
    "\n",
    "Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.\n",
    "cs.CR (Cryptography and Security)\n",
    "\n",
    "Covers all areas of cryptography and security including authentication, public key cryptosytems, proof-carrying code, etc. Roughly includes material in ACM Subject Classes D.4.6 and E.3.\n",
    "cs.CV (Computer Vision and Pattern Recognition)\n",
    "\n",
    "Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.\n",
    "cs.CY (Computers and Society)\n",
    "\n",
    "Covers impact of computers on society, computer ethics, information technology and public policy, legal aspects of computing, computers and education. Roughly includes material in ACM Subject Classes K.0, K.2, K.3, K.4, K.5, and K.7.\n",
    "cs.DB (Databases)\n",
    "\n",
    "Covers database management, datamining, and data processing. Roughly includes material in ACM Subject Classes E.2, E.5, H.0, H.2, and J.1.\n",
    "cs.DC (Distributed, Parallel, and Cluster Computing)\n",
    "\n",
    "Covers fault-tolerance, distributed algorithms, stabilility, parallel computation, and cluster computing. Roughly includes material in ACM Subject Classes C.1.2, C.1.4, C.2.4, D.1.3, D.4.5, D.4.7, E.1.\n",
    "cs.DL (Digital Libraries)\n",
    "\n",
    "Covers all aspects of the digital library design and document and text creation. Note that there will be some overlap with Information Retrieval (which is a separate subject area). Roughly includes material in ACM Subject Classes H.3.5, H.3.6, H.3.7, I.7.\n",
    "cs.DM (Discrete Mathematics)\n",
    "\n",
    "Covers combinatorics, graph theory, applications of probability. Roughly includes material in ACM Subject Classes G.2 and G.3.\n",
    "cs.DS (Data Structures and Algorithms)\n",
    "\n",
    "Covers data structures and analysis of algorithms. Roughly includes material in ACM Subject Classes E.1, E.2, F.2.1, and F.2.2.\n",
    "cs.ET (Emerging Technologies)\n",
    "\n",
    "Covers approaches to information processing (computing, communication, sensing) and bio-chemical analysis based on alternatives to silicon CMOS-based technologies, such as nanoscale electronic, photonic, spin-based, superconducting, mechanical, bio-chemical and quantum technologies (this list is not exclusive). Topics of interest include (1) building blocks for emerging technologies, their scalability and adoption in larger systems, including integration with traditional technologies, (2) modeling, design and optimization of novel devices and systems, (3) models of computation, algorithm design and programming for emerging technologies.\n",
    "cs.FL (Formal Languages and Automata Theory)\n",
    "\n",
    "Covers automata theory, formal language theory, grammars, and combinatorics on words. This roughly corresponds to ACM Subject Classes F.1.1, and F.4.3. Papers dealing with computational complexity should go to cs.CC; papers dealing with logic should go to cs.LO.\n",
    "cs.GL (General Literature)\n",
    "\n",
    "Covers introductory material, survey material, predictions of future trends, biographies, and miscellaneous computer-science related material. Roughly includes all of ACM Subject Class A, except it does not include conference proceedings (which will be listed in the appropriate subject area).\n",
    "cs.GR (Graphics)\n",
    "\n",
    "Covers all aspects of computer graphics. Roughly includes material in all of ACM Subject Class I.3, except that I.3.5 is is likely to have Computational Geometry as the primary subject area.\n",
    "cs.GT (Computer Science and Game Theory)\n",
    "\n",
    "Covers all theoretical and applied aspects at the intersection of computer science and game theory, including work in mechanism design, learning in games (which may overlap with Learning), foundations of agent modeling in games (which may overlap with Multiagent systems), coordination, specification and formal methods for non-cooperative computational environments. The area also deals with applications of game theory to areas such as electronic commerce.\n",
    "cs.HC (Human-Computer Interaction)\n",
    "\n",
    "Covers human factors, user interfaces, and collaborative computing. Roughly includes material in ACM Subject Classes H.1.2 and all of H.5, except for H.5.1, which is more likely to have Multimedia as the primary subject area.\n",
    "cs.IR (Information Retrieval)\n",
    "\n",
    "Covers indexing, dictionaries, retrieval, content and analysis. Roughly includes material in ACM Subject Classes H.3.0, H.3.1, H.3.2, H.3.3, and H.3.4.\n",
    "cs.IT (Information Theory)\n",
    "\n",
    "Covers theoretical and experimental aspects of information theory and coding. Includes material in ACM Subject Class E.4 and intersects with H.1.1.\n",
    "cs.LG (Machine Learning)\n",
    "\n",
    "Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.\n",
    "cs.LO (Logic in Computer Science)\n",
    "\n",
    "Covers all aspects of logic in computer science, including finite model theory, logics of programs, modal logic, and program verification. Programming language semantics should have Programming Languages as the primary subject area. Roughly includes material in ACM Subject Classes D.2.4, F.3.1, F.4.0, F.4.1, and F.4.2; some material in F.4.3 (formal languages) may also be appropriate here, although Computational Complexity is typically the more appropriate subject area.\n",
    "cs.MA (Multiagent Systems)\n",
    "\n",
    "Covers multiagent systems, distributed artificial intelligence, intelligent agents, coordinated interactions. and practical applications. Roughly covers ACM Subject Class I.2.11.\n",
    "cs.MM (Multimedia)\n",
    "\n",
    "Roughly includes material in ACM Subject Class H.5.1.\n",
    "cs.MS (Mathematical Software)\n",
    "\n",
    "Roughly includes material in ACM Subject Class G.4.\n",
    "cs.NA (Numerical Analysis)\n",
    "\n",
    "cs.NA is an alias for math.NA. Roughly includes material in ACM Subject Class G.1.\n",
    "cs.NE (Neural and Evolutionary Computing)\n",
    "\n",
    "Covers neural networks, connectionism, genetic algorithms, artificial life, adaptive behavior. Roughly includes some material in ACM Subject Class C.1.3, I.2.6, I.5.\n",
    "cs.NI (Networking and Internet Architecture)\n",
    "\n",
    "Covers all aspects of computer communication networks, including network architecture and design, network protocols, and internetwork standards (like TCP/IP). Also includes topics, such as web caching, that are directly relevant to Internet architecture and performance. Roughly includes all of ACM Subject Class C.2 except C.2.4, which is more likely to have Distributed, Parallel, and Cluster Computing as the primary subject area.\n",
    "cs.OH (Other Computer Science)\n",
    "\n",
    "This is the classification to use for documents that do not fit anywhere else.\n",
    "cs.OS (Operating Systems)\n",
    "\n",
    "Roughly includes material in ACM Subject Classes D.4.1, D.4.2., D.4.3, D.4.4, D.4.5, D.4.7, and D.4.9.\n",
    "cs.PF (Performance)\n",
    "\n",
    "Covers performance measurement and evaluation, queueing, and simulation. Roughly includes material in ACM Subject Classes D.4.8 and K.6.2.\n",
    "cs.PL (Programming Languages)\n",
    "\n",
    "Covers programming language semantics, language features, programming approaches (such as object-oriented programming, functional programming, logic programming). Also includes material on compilers oriented towards programming languages; other material on compilers may be more appropriate in Architecture (AR). Roughly includes material in ACM Subject Classes D.1 and D.3.\n",
    "cs.RO (Robotics)\n",
    "\n",
    "Roughly includes material in ACM Subject Class I.2.9.\n",
    "cs.SC (Symbolic Computation)\n",
    "\n",
    "Roughly includes material in ACM Subject Class I.1.\n",
    "cs.SD (Sound)\n",
    "\n",
    "Covers all aspects of computing with sound, and sound as an information channel. Includes models of sound, analysis and synthesis, audio user interfaces, sonification of data, computer music, and sound signal processing. Includes ACM Subject Class H.5.5, and intersects with H.1.2, H.5.1, H.5.2, I.2.7, I.5.4, I.6.3, J.5, K.4.2.\n",
    "cs.SE (Software Engineering)\n",
    "\n",
    "Covers design tools, software metrics, testing and debugging, programming environments, etc. Roughly includes material in all of ACM Subject Classes D.2, except that D.2.4 (program verification) should probably have Logics in Computer Science as the primary subject area.\n",
    "cs.SI (Social and Information Networks)\n",
    "\n",
    "Covers the design, analysis, and modeling of social and information networks, including their applications for on-line information access, communication, and interaction, and their roles as datasets in the exploration of questions in these and other domains, including connections to the social and biological sciences. Analysis and modeling of such networks includes topics in ACM Subject classes F.2, G.2, G.3, H.2, and I.2; applications in computing include topics in H.3, H.4, and H.5; and applications at the interface of computing and other disciplines include topics in J.1--J.7. Papers on computer communication systems and network protocols (e.g. TCP/IP) are generally a closer fit to the Networking and Internet Architecture (cs.NI) category.\n",
    "cs.SY (Systems and Control)\n",
    "\n",
    "cs.SY is an alias for eess.SY. This section includes theoretical and experimental research covering all facets of automatic control systems. The section is focused on methods of control system analysis and design using tools of modeling, simulation and optimization. Specific areas of research include nonlinear, distributed, adaptive, stochastic and robust control in addition to hybrid and discrete event systems. Application areas include automotive and aerospace control systems, network control, biological systems, multiagent and cooperative control, robotics, reinforcement learning, sensor networks, control of cyber-physical and energy-related systems, and control of computing systems.\n",
    "Economics\n",
    "econ.EM (Econometrics)\n",
    "\n",
    "Econometric Theory, Micro-Econometrics, Macro-Econometrics, Empirical Content of Economic Relations discovered via New Methods, Methodological Aspects of the Application of Statistical Inference to Economic Data.\n",
    "econ.GN (General Economics)\n",
    "\n",
    "General methodological, applied, and empirical contributions to economics.\n",
    "econ.TH (Theoretical Economics)\n",
    "\n",
    "Includes theoretical contributions to Contract Theory, Decision Theory, Game Theory, General Equilibrium, Growth, Learning and Evolution, Macroeconomics, Market and Mechanism Design, and Social Choice.\n",
    "Electrical Engineering and Systems Science\n",
    "eess.AS (Audio and Speech Processing)\n",
    "\n",
    "Theory and methods for processing signals representing audio, speech, and language, and their applications. This includes analysis, synthesis, enhancement, transformation, classification and interpretation of such signals as well as the design, development, and evaluation of associated signal processing systems. Machine learning and pattern analysis applied to any of the above areas is also welcome. Specific topics of interest include: auditory modeling and hearing aids; acoustic beamforming and source localization; classification of acoustic scenes; speaker separation; active noise control and echo cancellation; enhancement; de-reverberation; bioacoustics; music signals analysis, synthesis and modification; music information retrieval; audio for multimedia and joint audio-video processing; spoken and written language modeling, segmentation, tagging, parsing, understanding, and translation; text mining; speech production, perception, and psychoacoustics; speech analysis, synthesis, and perceptual modeling and coding; robust speech recognition; speaker recognition and characterization; deep learning, online learning, and graphical models applied to speech, audio, and language signals; and implementation aspects ranging from system architecture to fast algorithms.\n",
    "eess.IV (Image and Video Processing)\n",
    "\n",
    "Theory, algorithms, and architectures for the formation, capture, processing, communication, analysis, and display of images, video, and multidimensional signals in a wide variety of applications. Topics of interest include: mathematical, statistical, and perceptual image and video modeling and representation; linear and nonlinear filtering, de-blurring, enhancement, restoration, and reconstruction from degraded, low-resolution or tomographic data; lossless and lossy compression and coding; segmentation, alignment, and recognition; image rendering, visualization, and printing; computational imaging, including ultrasound, tomographic and magnetic resonance imaging; and image and video analysis, synthesis, storage, search and retrieval.\n",
    "eess.SP (Signal Processing)\n",
    "\n",
    "Theory, algorithms, performance analysis and applications of signal and data analysis, including physical modeling, processing, detection and parameter estimation, learning, mining, retrieval, and information extraction. The term \"signal\" includes speech, audio, sonar, radar, geophysical, physiological, (bio-) medical, image, video, and multimodal natural and man-made signals, including communication signals and data. Topics of interest include: statistical signal processing, spectral estimation and system identification; filter design, adaptive filtering / stochastic learning; (compressive) sampling, sensing, and transform-domain methods including fast algorithms; signal processing for machine learning and machine learning for signal processing applications; in-network and graph signal processing; convex and nonconvex optimization methods for signal processing applications; radar, sonar, and sensor array beamforming and direction finding; communications signal processing; low power, multi-core and system-on-chip signal processing; sensing, communication, analysis and optimization for cyber-physical systems such as power grids and the Internet of Things.\n",
    "eess.SY (Systems and Control)\n",
    "\n",
    "This section includes theoretical and experimental research covering all facets of automatic control systems. The section is focused on methods of control system analysis and design using tools of modeling, simulation and optimization. Specific areas of research include nonlinear, distributed, adaptive, stochastic and robust control in addition to hybrid and discrete event systems. Application areas include automotive and aerospace control systems, network control, biological systems, multiagent and cooperative control, robotics, reinforcement learning, sensor networks, control of cyber-physical and energy-related systems, and control of computing systems.\n",
    "Mathematics\n",
    "math.AC (Commutative Algebra)\n",
    "\n",
    "Commutative rings, modules, ideals, homological algebra, computational aspects, invariant theory, connections to algebraic geometry and combinatorics\n",
    "math.AG (Algebraic Geometry)\n",
    "\n",
    "Algebraic varieties, stacks, sheaves, schemes, moduli spaces, complex geometry, quantum cohomology\n",
    "math.AP (Analysis of PDEs)\n",
    "\n",
    "Existence and uniqueness, boundary conditions, linear and non-linear operators, stability, soliton theory, integrable PDE's, conservation laws, qualitative dynamics\n",
    "math.AT (Algebraic Topology)\n",
    "\n",
    "Homotopy theory, homological algebra, algebraic treatments of manifolds\n",
    "math.CA (Classical Analysis and ODEs)\n",
    "\n",
    "Special functions, orthogonal polynomials, harmonic analysis, ODE's, differential relations, calculus of variations, approximations, expansions, asymptotics\n",
    "math.CO (Combinatorics)\n",
    "\n",
    "Discrete mathematics, graph theory, enumeration, combinatorial optimization, Ramsey theory, combinatorial game theory\n",
    "math.CT (Category Theory)\n",
    "\n",
    "Enriched categories, topoi, abelian categories, monoidal categories, homological algebra\n",
    "math.CV (Complex Variables)\n",
    "\n",
    "Holomorphic functions, automorphic group actions and forms, pseudoconvexity, complex geometry, analytic spaces, analytic sheaves\n",
    "math.DG (Differential Geometry)\n",
    "\n",
    "Complex, contact, Riemannian, pseudo-Riemannian and Finsler geometry, relativity, gauge theory, global analysis\n",
    "math.DS (Dynamical Systems)\n",
    "\n",
    "Dynamics of differential equations and flows, mechanics, classical few-body problems, iterations, complex dynamics, delayed differential equations\n",
    "math.FA (Functional Analysis)\n",
    "\n",
    "Banach spaces, function spaces, real functions, integral transforms, theory of distributions, measure theory\n",
    "math.GM (General Mathematics)\n",
    "\n",
    "Mathematical material of general interest, topics not covered elsewhere\n",
    "math.GN (General Topology)\n",
    "\n",
    "Continuum theory, point-set topology, spaces with algebraic structure, foundations, dimension theory, local and global properties\n",
    "math.GR (Group Theory)\n",
    "\n",
    "Finite groups, topological groups, representation theory, cohomology, classification and structure\n",
    "math.GT (Geometric Topology)\n",
    "\n",
    "Manifolds, orbifolds, polyhedra, cell complexes, foliations, geometric structures\n",
    "math.HO (History and Overview)\n",
    "\n",
    "Biographies, philosophy of mathematics, mathematics education, recreational mathematics, communication of mathematics, ethics in mathematics\n",
    "math.IT (Information Theory)\n",
    "\n",
    "math.IT is an alias for cs.IT. Covers theoretical and experimental aspects of information theory and coding.\n",
    "math.KT (K-Theory and Homology)\n",
    "\n",
    "Algebraic and topological K-theory, relations with topology, commutative algebra, and operator algebras\n",
    "math.LO (Logic)\n",
    "\n",
    "Logic, set theory, point-set topology, formal mathematics\n",
    "math.MG (Metric Geometry)\n",
    "\n",
    "Euclidean, hyperbolic, discrete, convex, coarse geometry, comparisons in Riemannian geometry, symmetric spaces\n",
    "math.MP (Mathematical Physics)\n",
    "\n",
    "math.MP is an alias for math-ph. Articles in this category focus on areas of research that illustrate the application of mathematics to problems in physics, develop mathematical methods for such applications, or provide mathematically rigorous formulations of existing physical theories. Submissions to math-ph should be of interest to both physically oriented mathematicians and mathematically oriented physicists; submissions which are primarily of interest to theoretical physicists or to mathematicians should probably be directed to the respective physics/math categories\n",
    "math.NA (Numerical Analysis)\n",
    "\n",
    "Numerical algorithms for problems in analysis and algebra, scientific computation\n",
    "math.NT (Number Theory)\n",
    "\n",
    "Prime numbers, diophantine equations, analytic number theory, algebraic number theory, arithmetic geometry, Galois theory\n",
    "math.OA (Operator Algebras)\n",
    "\n",
    "Algebras of operators on Hilbert space, C^*-algebras, von Neumann algebras, non-commutative geometry\n",
    "math.OC (Optimization and Control)\n",
    "\n",
    "Operations research, linear programming, control theory, systems theory, optimal control, game theory\n",
    "math.PR (Probability)\n",
    "\n",
    "Theory and applications of probability and stochastic processes: e.g. central limit theorems, large deviations, stochastic differential equations, models from statistical mechanics, queuing theory\n",
    "math.QA (Quantum Algebra)\n",
    "\n",
    "Quantum groups, skein theories, operadic and diagrammatic algebra, quantum field theory\n",
    "math.RA (Rings and Algebras)\n",
    "\n",
    "Non-commutative rings and algebras, non-associative algebras, universal algebra and lattice theory, linear algebra, semigroups\n",
    "math.RT (Representation Theory)\n",
    "\n",
    "Linear representations of algebras and groups, Lie theory, associative algebras, multilinear algebra\n",
    "math.SG (Symplectic Geometry)\n",
    "\n",
    "Hamiltonian systems, symplectic flows, classical integrable systems\n",
    "math.SP (Spectral Theory)\n",
    "\n",
    "Schrodinger operators, operators on manifolds, general differential operators, numerical studies, integral operators, discrete models, resonances, non-self-adjoint operators, random operators/matrices\n",
    "math.ST (Statistics Theory)\n",
    "\n",
    "Applied, computational and theoretical statistics: e.g. statistical inference, regression, time series, multivariate analysis, data analysis, Markov chain Monte Carlo, design of experiments, case studies\n",
    "Physics\n",
    "Astrophysics\n",
    "(astro-ph)\n",
    "astro-ph.CO (Cosmology and Nongalactic Astrophysics)\n",
    "\n",
    "Phenomenology of early universe, cosmic microwave background, cosmological parameters, primordial element abundances, extragalactic distance scale, large-scale structure of the universe. Groups, superclusters, voids, intergalactic medium. Particle astrophysics: dark energy, dark matter, baryogenesis, leptogenesis, inflationary models, reheating, monopoles, WIMPs, cosmic strings, primordial black holes, cosmological gravitational radiation\n",
    "astro-ph.EP (Earth and Planetary Astrophysics)\n",
    "\n",
    "Interplanetary medium, planetary physics, planetary astrobiology, extrasolar planets, comets, asteroids, meteorites. Structure and formation of the solar system\n",
    "astro-ph.GA (Astrophysics of Galaxies)\n",
    "\n",
    "Phenomena pertaining to galaxies or the Milky Way. Star clusters, HII regions and planetary nebulae, the interstellar medium, atomic and molecular clouds, dust. Stellar populations. Galactic structure, formation, dynamics. Galactic nuclei, bulges, disks, halo. Active Galactic Nuclei, supermassive black holes, quasars. Gravitational lens systems. The Milky Way and its contents\n",
    "astro-ph.HE (High Energy Astrophysical Phenomena)\n",
    "\n",
    "Cosmic ray production, acceleration, propagation, detection. Gamma ray astronomy and bursts, X-rays, charged particles, supernovae and other explosive phenomena, stellar remnants and accretion systems, jets, microquasars, neutron stars, pulsars, black holes\n",
    "astro-ph.IM (Instrumentation and Methods for Astrophysics)\n",
    "\n",
    "Detector and telescope design, experiment proposals. Laboratory Astrophysics. Methods for data analysis, statistical methods. Software, database design\n",
    "astro-ph.SR (Solar and Stellar Astrophysics)\n",
    "\n",
    "White dwarfs, brown dwarfs, cataclysmic variables. Star formation and protostellar systems, stellar astrobiology, binary and multiple systems of stars, stellar evolution and structure, coronas. Central stars of planetary nebulae. Helioseismology, solar neutrinos, production and detection of gravitational radiation from stellar systems\n",
    "Condensed Matter\n",
    "(cond-mat)\n",
    "cond-mat.dis-nn (Disordered Systems and Neural Networks)\n",
    "\n",
    "Glasses and spin glasses; properties of random, aperiodic and quasiperiodic systems; transport in disordered media; localization; phenomena mediated by defects and disorder; neural networks\n",
    "cond-mat.mes-hall (Mesoscale and Nanoscale Physics)\n",
    "\n",
    "Semiconducting nanostructures: quantum dots, wires, and wells. Single electronics, spintronics, 2d electron gases, quantum Hall effect, nanotubes, graphene, plasmonic nanostructures\n",
    "cond-mat.mtrl-sci (Materials Science)\n",
    "\n",
    "Techniques, synthesis, characterization, structure. Structural phase transitions, mechanical properties, phonons. Defects, adsorbates, interfaces\n",
    "cond-mat.other (Other Condensed Matter)\n",
    "\n",
    "Work in condensed matter that does not fit into the other cond-mat classifications\n",
    "cond-mat.quant-gas (Quantum Gases)\n",
    "\n",
    "Ultracold atomic and molecular gases, Bose-Einstein condensation, Feshbach resonances, spinor condensates, optical lattices, quantum simulation with cold atoms and molecules, macroscopic interference phenomena\n",
    "cond-mat.soft (Soft Condensed Matter)\n",
    "\n",
    "Membranes, polymers, liquid crystals, glasses, colloids, granular matter\n",
    "cond-mat.stat-mech (Statistical Mechanics)\n",
    "\n",
    "Phase transitions, thermodynamics, field theory, non-equilibrium phenomena, renormalization group and scaling, integrable models, turbulence\n",
    "cond-mat.str-el (Strongly Correlated Electrons)\n",
    "\n",
    "Quantum magnetism, non-Fermi liquids, spin liquids, quantum criticality, charge density waves, metal-insulator transitions\n",
    "cond-mat.supr-con (Superconductivity)\n",
    "\n",
    "Superconductivity: theory, models, experiment. Superflow in helium\n",
    "General Relativity and Quantum Cosmology\n",
    "(gr-qc)\n",
    "gr-qc (General Relativity and Quantum Cosmology)\n",
    "\n",
    "General Relativity and Quantum Cosmology Areas of gravitational physics, including experiments and observations related to the detection and interpretation of gravitational waves, experimental tests of gravitational theories, computational general relativity, relativistic astrophysics, solutions to Einstein's equations and their properties, alternative theories of gravity, classical and quantum cosmology, and quantum gravity.\n",
    "High Energy Physics - Experiment\n",
    "(hep-ex)\n",
    "hep-ex (High Energy Physics - Experiment)\n",
    "\n",
    "Description coming soon\n",
    "High Energy Physics - Lattice\n",
    "(hep-lat)\n",
    "hep-lat (High Energy Physics - Lattice)\n",
    "\n",
    "Lattice field theory. Phenomenology from lattice field theory. Algorithms for lattice field theory. Hardware for lattice field theory.\n",
    "High Energy Physics - Phenomenology\n",
    "(hep-ph)\n",
    "hep-ph (High Energy Physics - Phenomenology)\n",
    "\n",
    "Theoretical particle physics and its interrelation with experiment. Prediction of particle physics observables: models, effective field theories, calculation techniques. Particle physics: analysis of theory through experimental results.\n",
    "High Energy Physics - Theory\n",
    "(hep-th)\n",
    "hep-th (High Energy Physics - Theory)\n",
    "\n",
    "Formal aspects of quantum field theory. String theory, supersymmetry and supergravity.\n",
    "Mathematical Physics\n",
    "(math-ph)\n",
    "math-ph (Mathematical Physics)\n",
    "\n",
    "Articles in this category focus on areas of research that illustrate the application of mathematics to problems in physics, develop mathematical methods for such applications, or provide mathematically rigorous formulations of existing physical theories. Submissions to math-ph should be of interest to both physically oriented mathematicians and mathematically oriented physicists; submissions which are primarily of interest to theoretical physicists or to mathematicians should probably be directed to the respective physics/math categories\n",
    "Nonlinear Sciences\n",
    "(nlin)\n",
    "nlin.AO (Adaptation and Self-Organizing Systems)\n",
    "\n",
    "Adaptation, self-organizing systems, statistical physics, fluctuating systems, stochastic processes, interacting particle systems, machine learning\n",
    "nlin.CD (Chaotic Dynamics)\n",
    "\n",
    "Dynamical systems, chaos, quantum chaos, topological dynamics, cycle expansions, turbulence, propagation\n",
    "nlin.CG (Cellular Automata and Lattice Gases)\n",
    "\n",
    "Computational methods, time series analysis, signal processing, wavelets, lattice gases\n",
    "nlin.PS (Pattern Formation and Solitons)\n",
    "\n",
    "Pattern formation, coherent structures, solitons\n",
    "nlin.SI (Exactly Solvable and Integrable Systems)\n",
    "\n",
    "Exactly solvable systems, integrable PDEs, integrable ODEs, Painleve analysis, integrable discrete maps, solvable lattice models, integrable quantum systems\n",
    "Nuclear Experiment\n",
    "(nucl-ex)\n",
    "nucl-ex (Nuclear Experiment)\n",
    "\n",
    "Nuclear Experiment Results from experimental nuclear physics including the areas of fundamental interactions, measurements at low- and medium-energy, as well as relativistic heavy-ion collisions. Does not include: detectors and instrumentation nor analysis methods to conduct experiments; descriptions of experimental programs (present or future); comments on published results\n",
    "Nuclear Theory\n",
    "(nucl-th)\n",
    "nucl-th (Nuclear Theory)\n",
    "\n",
    "Nuclear Theory Theory of nuclear structure covering wide area from models of hadron structure to neutron stars. Nuclear equation of states at different external conditions. Theory of nuclear reactions including heavy-ion reactions at low and high energies. It does not include problems of data analysis, physics of nuclear reactors, problems of safety, reactor construction\n",
    "Physics\n",
    "(physics)\n",
    "physics.acc-ph (Accelerator Physics)\n",
    "\n",
    "Accelerator theory and simulation. Accelerator technology. Accelerator experiments. Beam Physics. Accelerator design and optimization. Advanced accelerator concepts. Radiation sources including synchrotron light sources and free electron lasers. Applications of accelerators.\n",
    "physics.ao-ph (Atmospheric and Oceanic Physics)\n",
    "\n",
    "Atmospheric and oceanic physics and physical chemistry, biogeophysics, and climate science\n",
    "physics.app-ph (Applied Physics)\n",
    "\n",
    "Applications of physics to new technology, including electronic devices, optics, photonics, microwaves, spintronics, advanced materials, metamaterials, nanotechnology, and energy sciences.\n",
    "physics.atm-clus (Atomic and Molecular Clusters)\n",
    "\n",
    "Atomic and molecular clusters, nanoparticles: geometric, electronic, optical, chemical, magnetic properties, shell structure, phase transitions, optical spectroscopy, mass spectrometry, photoelectron spectroscopy, ionization potential, electron affinity, interaction with intense light pulses, electron diffraction, light scattering, ab initio calculations, DFT theory, fragmentation, Coulomb explosion, hydrodynamic expansion.\n",
    "physics.atom-ph (Atomic Physics)\n",
    "\n",
    "Atomic and molecular structure, spectra, collisions, and data. Atoms and molecules in external fields. Molecular dynamics and coherent and optical control. Cold atoms and molecules. Cold collisions. Optical lattices.\n",
    "physics.bio-ph (Biological Physics)\n",
    "\n",
    "Molecular biophysics, cellular biophysics, neurological biophysics, membrane biophysics, single-molecule biophysics, ecological biophysics, quantum phenomena in biological systems (quantum biophysics), theoretical biophysics, molecular dynamics/modeling and simulation, game theory, biomechanics, bioinformatics, microorganisms, virology, evolution, biophysical methods.\n",
    "physics.chem-ph (Chemical Physics)\n",
    "\n",
    "Experimental, computational, and theoretical physics of atoms, molecules, and clusters - Classical and quantum description of states, processes, and dynamics; spectroscopy, electronic structure, conformations, reactions, interactions, and phases. Chemical thermodynamics. Disperse systems. High pressure chemistry. Solid state chemistry. Surface and interface chemistry.\n",
    "physics.class-ph (Classical Physics)\n",
    "\n",
    "Newtonian and relativistic dynamics; many particle systems; planetary motions; chaos in classical dynamics. Maxwell's equations and dynamics of charged systems and electromagnetic forces in materials. Vibrating systems such as membranes and cantilevers; optomechanics. Classical waves, including acoustics and elasticity; physics of music and musical instruments. Classical thermodynamics and heat flow problems.\n",
    "physics.comp-ph (Computational Physics)\n",
    "\n",
    "All aspects of computational science applied to physics.\n",
    "physics.data-an (Data Analysis, Statistics and Probability)\n",
    "\n",
    "Methods, software and hardware for physics data analysis: data processing and storage; measurement methodology; statistical and mathematical aspects such as parametrization and uncertainties.\n",
    "physics.ed-ph (Physics Education)\n",
    "\n",
    "Report of results of a research study, laboratory experience, assessment or classroom practice that represents a way to improve teaching and learning in physics. Also, report on misconceptions of students, textbook errors, and other similar information relative to promoting physics understanding.\n",
    "physics.flu-dyn (Fluid Dynamics)\n",
    "\n",
    "Turbulence, instabilities, incompressible/compressible flows, reacting flows. Aero/hydrodynamics, fluid-structure interactions, acoustics. Biological fluid dynamics, micro/nanofluidics, interfacial phenomena. Complex fluids, suspensions and granular flows, porous media flows. Geophysical flows, thermoconvective and stratified flows. Mathematical and computational methods for fluid dynamics, fluid flow models, experimental techniques.\n",
    "physics.gen-ph (General Physics)\n",
    "\n",
    "Description coming soon\n",
    "physics.geo-ph (Geophysics)\n",
    "\n",
    "Atmospheric physics. Biogeosciences. Computational geophysics. Geographic location. Geoinformatics. Geophysical techniques. Hydrospheric geophysics. Magnetospheric physics. Mathematical geophysics. Planetology. Solar system. Solid earth geophysics. Space plasma physics. Mineral physics. High pressure physics.\n",
    "physics.hist-ph (History and Philosophy of Physics)\n",
    "\n",
    "History and philosophy of all branches of physics, astrophysics, and cosmology, including appreciations of physicists.\n",
    "physics.ins-det (Instrumentation and Detectors)\n",
    "\n",
    "Instrumentation and Detectors for research in natural science, including optical, molecular, atomic, nuclear and particle physics instrumentation and the associated electronics, services, infrastructure and control equipment.\n",
    "physics.med-ph (Medical Physics)\n",
    "\n",
    "Radiation therapy. Radiation dosimetry. Biomedical imaging modelling. Reconstruction, processing, and analysis. Biomedical system modelling and analysis. Health physics. New imaging or therapy modalities.\n",
    "physics.optics (Optics)\n",
    "\n",
    "Adaptive optics. Astronomical optics. Atmospheric optics. Biomedical optics. Cardinal points. Collimation. Doppler effect. Fiber optics. Fourier optics. Geometrical optics (Gradient index optics. Holography. Infrared optics. Integrated optics. Laser applications. Laser optical systems. Lasers. Light amplification. Light diffraction. Luminescence. Microoptics. Nano optics. Ocean optics. Optical computing. Optical devices. Optical imaging. Optical materials. Optical metrology. Optical microscopy. Optical properties. Optical signal processing. Optical testing techniques. Optical wave propagation. Paraxial optics. Photoabsorption. Photoexcitations. Physical optics. Physiological optics. Quantum optics. Segmented optics. Spectra. Statistical optics. Surface optics. Ultrafast optics. Wave optics. X-ray optics.\n",
    "physics.plasm-ph (Plasma Physics)\n",
    "\n",
    "Fundamental plasma physics. Magnetically Confined Plasmas (includes magnetic fusion energy research). High Energy Density Plasmas (inertial confinement plasmas, laser-plasma interactions). Ionospheric, Heliophysical, and Astrophysical plasmas (includes sun and solar system plasmas). Lasers, Accelerators, and Radiation Generation. Low temperature plasmas and plasma applications (include dusty plasmas, semiconductor etching, plasma-based nanotechnology, medical applications). Plasma Diagnostics, Engineering and Enabling Technologies (includes fusion reactor design, heating systems, diagnostics, experimental techniques)\n",
    "physics.pop-ph (Popular Physics)\n",
    "\n",
    "Description coming soon\n",
    "physics.soc-ph (Physics and Society)\n",
    "\n",
    "Structure, dynamics and collective behavior of societies and groups (human or otherwise). Quantitative analysis of social networks and other complex networks. Physics and engineering of infrastructure and systems of broad societal impact (e.g., energy grids, transportation networks).\n",
    "physics.space-ph (Space Physics)\n",
    "\n",
    "Space plasma physics. Heliophysics. Space weather. Planetary magnetospheres, ionospheres and magnetotail. Auroras. Interplanetary space. Cosmic rays. Synchrotron radiation. Radio astronomy.\n",
    "Quantum Physics\n",
    "(quant-ph)\n",
    "quant-ph (Quantum Physics)\n",
    "\n",
    "Description coming soon\n",
    "Quantitative Biology\n",
    "q-bio.BM (Biomolecules)\n",
    "\n",
    "DNA, RNA, proteins, lipids, etc.; molecular structures and folding kinetics; molecular interactions; single-molecule manipulation.\n",
    "q-bio.CB (Cell Behavior)\n",
    "\n",
    "Cell-cell signaling and interaction; morphogenesis and development; apoptosis; bacterial conjugation; viral-host interaction; immunology\n",
    "q-bio.GN (Genomics)\n",
    "\n",
    "DNA sequencing and assembly; gene and motif finding; RNA editing and alternative splicing; genomic structure and processes (replication, transcription, methylation, etc); mutational processes.\n",
    "q-bio.MN (Molecular Networks)\n",
    "\n",
    "Gene regulation, signal transduction, proteomics, metabolomics, gene and enzymatic networks\n",
    "q-bio.NC (Neurons and Cognition)\n",
    "\n",
    "Synapse, cortex, neuronal dynamics, neural network, sensorimotor control, behavior, attention\n",
    "q-bio.OT (Other Quantitative Biology)\n",
    "\n",
    "Work in quantitative biology that does not fit into the other q-bio classifications\n",
    "q-bio.PE (Populations and Evolution)\n",
    "\n",
    "Population dynamics, spatio-temporal and epidemiological models, dynamic speciation, co-evolution, biodiversity, foodwebs, aging; molecular evolution and phylogeny; directed evolution; origin of life\n",
    "q-bio.QM (Quantitative Methods)\n",
    "\n",
    "All experimental, numerical, statistical and mathematical contributions of value to biology\n",
    "q-bio.SC (Subcellular Processes)\n",
    "\n",
    "Assembly and control of subcellular structures (channels, organelles, cytoskeletons, capsules, etc.); molecular motors, transport, subcellular localization; mitosis and meiosis\n",
    "q-bio.TO (Tissues and Organs)\n",
    "\n",
    "Blood flow in vessels, biomechanics of bones, electrical waves, endocrine system, tumor growth\n",
    "Quantitative Finance\n",
    "q-fin.CP (Computational Finance)\n",
    "\n",
    "Computational methods, including Monte Carlo, PDE, lattice and other numerical methods with applications to financial modeling\n",
    "q-fin.EC (Economics)\n",
    "\n",
    "q-fin.EC is an alias for econ.GN. Economics, including micro and macro economics, international economics, theory of the firm, labor economics, and other economic topics outside finance\n",
    "q-fin.GN (General Finance)\n",
    "\n",
    "Development of general quantitative methodologies with applications in finance\n",
    "q-fin.MF (Mathematical Finance)\n",
    "\n",
    "Mathematical and analytical methods of finance, including stochastic, probabilistic and functional analysis, algebraic, geometric and other methods\n",
    "q-fin.PM (Portfolio Management)\n",
    "\n",
    "Security selection and optimization, capital allocation, investment strategies and performance measurement\n",
    "q-fin.PR (Pricing of Securities)\n",
    "\n",
    "Valuation and hedging of financial securities, their derivatives, and structured products\n",
    "q-fin.RM (Risk Management)\n",
    "\n",
    "Measurement and management of financial risks in trading, banking, insurance, corporate and other applications\n",
    "q-fin.ST (Statistical Finance)\n",
    "\n",
    "Statistical, econometric and econophysics analyses with applications to financial markets and economic data\n",
    "q-fin.TR (Trading and Market Microstructure)\n",
    "\n",
    "Market microstructure, liquidity, exchange and auction design, automated trading, agent-based modeling and market-making\n",
    "Statistics\n",
    "stat.AP (Applications)\n",
    "\n",
    "Biology, Education, Epidemiology, Engineering, Environmental Sciences, Medical, Physical Sciences, Quality Control, Social Sciences\n",
    "stat.CO (Computation)\n",
    "\n",
    "Algorithms, Simulation, Visualization\n",
    "stat.ME (Methodology)\n",
    "\n",
    "Design, Surveys, Model Selection, Multiple Testing, Multivariate Methods, Signal and Image Processing, Time Series, Smoothing, Spatial Statistics, Survival Analysis, Nonparametric and Semiparametric Methods\n",
    "stat.ML (Machine Learning)\n",
    "\n",
    "Covers machine learning papers (supervised, unsupervised, semi-supervised learning, graphical models, reinforcement learning, bandits, high dimensional inference, etc.) with a statistical or theoretical grounding\n",
    "stat.OT (Other Statistics)\n",
    "\n",
    "Work in statistics that does not fit into the other stat classifications\n",
    "stat.TH (Statistics Theory)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89123678-2d13-4fd2-b7cc-06004fa368a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cs', 'econ', 'eess', 'math', 'physics', 'q-bio', 'q-fin', 'stat'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import feedparser\n",
    "\n",
    "def find_all(s):\n",
    "    regex = r\"^[a-z]((\\w*(\\-\\w+)?\\.\\w+(\\-\\w+)?)|(\\w*\\-\\w+))\"\n",
    "    return [res[0] for line in s.split('\\n') if (res := re.match(regex, line))]\n",
    "\n",
    "themes = find_all(taxonomy)\n",
    "\n",
    "physics = {'quant-ph', 'nucl-th', 'nucl-ex', 'nlin', 'math-ph', 'hep-th', 'nlin',\n",
    "           'hep-ph', 'hep-lat', 'hep-ex', 'gr-qc', 'cond-mat', 'astro-ph', 'adap-org'}\n",
    "\n",
    "themes_classif = defaultdict(list)\n",
    "\n",
    "for theme in themes:\n",
    "    for subth in physics:\n",
    "        if theme.startswith(subth):\n",
    "            themes_classif[\"physics\"].append(theme)\n",
    "            break\n",
    "    else:\n",
    "        themes_classif[theme[:theme.find(\".\")]].append(theme)\n",
    "\n",
    "themes_classif.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91fee85a-c216-4884-9b26-029088d368cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2305.04475v1',\n",
       " 'guidislink': True,\n",
       " 'link': 'http://arxiv.org/abs/2305.04475v1',\n",
       " 'updated': '2023-05-08T05:54:29Z',\n",
       " 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=5, tm_min=54, tm_sec=29, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       " 'published': '2023-05-08T05:54:29Z',\n",
       " 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=5, tm_min=54, tm_sec=29, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       " 'title': 'Adaptive Learning Path Navigation Based on Knowledge Tracing and\\n  Reinforcement Learning',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': 'Adaptive Learning Path Navigation Based on Knowledge Tracing and\\n  Reinforcement Learning'},\n",
       " 'summary': \"This paper introduces the Adaptive Learning Path Navigation (ALPN) system, a\\nscalable approach for creating adaptive learning paths within E-learning\\nsystems. The ALPN system employs an attention-based Knowledge Tracing (AKT)\\nmodel to evaluate students' knowledge states and a decision-making model using\\nProximal Policy Optimization (PPO) to suggest customized learning materials.\\nThe proposed system accommodates students' needs by considering personalization\\nparameters such as learning objectives, time constraints, and knowledge\\nbackgrounds. Through an iterative process of recommendation and knowledge state\\nupdating, the ALPN system produces highly adaptive learning paths. Experimental\\nresults reveal the outstanding performance of the proposed system, providing\\ngood insights into the future development of E-learning systems.\",\n",
       " 'summary_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': \"This paper introduces the Adaptive Learning Path Navigation (ALPN) system, a\\nscalable approach for creating adaptive learning paths within E-learning\\nsystems. The ALPN system employs an attention-based Knowledge Tracing (AKT)\\nmodel to evaluate students' knowledge states and a decision-making model using\\nProximal Policy Optimization (PPO) to suggest customized learning materials.\\nThe proposed system accommodates students' needs by considering personalization\\nparameters such as learning objectives, time constraints, and knowledge\\nbackgrounds. Through an iterative process of recommendation and knowledge state\\nupdating, the ALPN system produces highly adaptive learning paths. Experimental\\nresults reveal the outstanding performance of the proposed system, providing\\ngood insights into the future development of E-learning systems.\"},\n",
       " 'authors': [{'name': 'Jyun-Yi Chen'},\n",
       "  {'name': 'Saeed Saeedvand'},\n",
       "  {'name': 'I-Wei Lai'}],\n",
       " 'author_detail': {'name': 'I-Wei Lai'},\n",
       " 'author': 'I-Wei Lai',\n",
       " 'links': [{'href': 'http://arxiv.org/abs/2305.04475v1',\n",
       "   'rel': 'alternate',\n",
       "   'type': 'text/html'},\n",
       "  {'title': 'pdf',\n",
       "   'href': 'http://arxiv.org/pdf/2305.04475v1',\n",
       "   'rel': 'related',\n",
       "   'type': 'application/pdf'}],\n",
       " 'arxiv_primary_category': {'term': 'cs.AI',\n",
       "  'scheme': 'http://arxiv.org/schemas/atom'},\n",
       " 'tags': [{'term': 'cs.AI',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'label': None},\n",
       "  {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['cs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d1046a62-a402-4827-9d20-2ed4aff0b7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adap-org',\n",
       " 'astro-ph',\n",
       " 'cond-mat',\n",
       " 'gr-qc',\n",
       " 'hep-ex',\n",
       " 'hep-lat',\n",
       " 'hep-ph',\n",
       " 'hep-th',\n",
       " 'math-ph',\n",
       " 'nlin',\n",
       " 'nucl-ex',\n",
       " 'nucl-th',\n",
       " 'quant-ph'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f5bdc57-a0d6-48f6-9c7e-3e516535c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_papers(papers):\n",
    "    papers_classif = defaultdict(lambda: defaultdict(list))\n",
    "    pap_merged = [paper for item in papers.values() for paper in item]\n",
    "    ids = set()\n",
    "    for paper in pap_merged:\n",
    "        if paper['id'] in ids:\n",
    "            continue\n",
    "        ids.add(paper['id'])\n",
    "        prim_cat = paper[\"arxiv_primary_category\"][\"term\"]\n",
    "        for subth in physics:\n",
    "            if prim_cat.startswith(subth):\n",
    "                cat = \"physics\"\n",
    "                break\n",
    "        else:\n",
    "            cat = prim_cat[:prim_cat.find(\".\")]\n",
    "        papers_classif[cat][prim_cat].append(paper)\n",
    "    return papers_classif, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "92637337-c4c8-4e8b-93e7-91bb8601c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "pap_merged_raw = [paper for item in papers.values() for paper in item]\n",
    "pap_merged = []\n",
    "ids = set()\n",
    "for paper in pap_merged_raw:\n",
    "    if paper['id'] in ids:\n",
    "        continue\n",
    "    ids.add(paper['id'])\n",
    "    pap_merged.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e93b7ce-c5e4-4171-bf16-9db7d32fd5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'http://arxiv.org/abs/2004.00944v3',\n",
       " 'guidislink': True,\n",
       " 'link': 'http://arxiv.org/abs/2004.00944v3',\n",
       " 'updated': '2020-08-17T10:00:29Z',\n",
       " 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=8, tm_mday=17, tm_hour=10, tm_min=0, tm_sec=29, tm_wday=0, tm_yday=230, tm_isdst=0),\n",
       " 'published': '2020-04-02T11:31:02Z',\n",
       " 'published_parsed': time.struct_time(tm_year=2020, tm_mon=4, tm_mday=2, tm_hour=11, tm_min=31, tm_sec=2, tm_wday=3, tm_yday=93, tm_isdst=0),\n",
       " 'title': 'Status hierarchy and group cooperation: A generalized model',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': 'Status hierarchy and group cooperation: A generalized model'},\n",
       " 'summary': \"In a refreshing mathematical investigation, Mark (2018) shows that status\\nhierarchy may facilitate the emergence of cooperation in groups. Despite the\\ncontribution, the present paper notes that there are limitations in Mark's\\nmodel that makes it less realistic than it could in explaining real-world\\nexperiences. Consequently, we present a more generalized modified framework in\\nwhich his model is a special case, by developing and introducing a new\\nhierarchy measure into the model to estimate the cooperation level in a set of\\nhierarchical structures omitted in Mark's work yet common in everyday\\nlife--those with multiple leaders. We derived the conditions under which\\ncooperation can emerge in these groups, and verified our analytical predictions\\nin agent-based computer simulations. In so doing, not only does our model\\nelaborate on its predecessor and support Mark's general prediction. For theory,\\nour work further reveals two novel phenomena of group cooperation: Both the\\nrelative number of cooperators to defectors in groups and the assortativity\\namong these different roles can backfire; they are not always the higher, the\\nbetter for cooperation to thrive. For methodology, the hierarchy measure\\ndeveloped and our model using the measure may also be applied in future\\nresearch on a wide range of related topics.\",\n",
       " 'summary_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': '',\n",
       "  'value': \"In a refreshing mathematical investigation, Mark (2018) shows that status\\nhierarchy may facilitate the emergence of cooperation in groups. Despite the\\ncontribution, the present paper notes that there are limitations in Mark's\\nmodel that makes it less realistic than it could in explaining real-world\\nexperiences. Consequently, we present a more generalized modified framework in\\nwhich his model is a special case, by developing and introducing a new\\nhierarchy measure into the model to estimate the cooperation level in a set of\\nhierarchical structures omitted in Mark's work yet common in everyday\\nlife--those with multiple leaders. We derived the conditions under which\\ncooperation can emerge in these groups, and verified our analytical predictions\\nin agent-based computer simulations. In so doing, not only does our model\\nelaborate on its predecessor and support Mark's general prediction. For theory,\\nour work further reveals two novel phenomena of group cooperation: Both the\\nrelative number of cooperators to defectors in groups and the assortativity\\namong these different roles can backfire; they are not always the higher, the\\nbetter for cooperation to thrive. For methodology, the hierarchy measure\\ndeveloped and our model using the measure may also be applied in future\\nresearch on a wide range of related topics.\"},\n",
       " 'authors': [{'name': 'Hsuan-Wei Lee'},\n",
       "  {'name': 'Yen-Ping Chang'},\n",
       "  {'name': 'Yen-Sheng Chiang'}],\n",
       " 'author_detail': {'name': 'Yen-Sheng Chiang'},\n",
       " 'author': 'Yen-Sheng Chiang',\n",
       " 'arxiv_comment': '42 pages, 11 figures',\n",
       " 'links': [{'href': 'http://arxiv.org/abs/2004.00944v3',\n",
       "   'rel': 'alternate',\n",
       "   'type': 'text/html'},\n",
       "  {'title': 'pdf',\n",
       "   'href': 'http://arxiv.org/pdf/2004.00944v3',\n",
       "   'rel': 'related',\n",
       "   'type': 'application/pdf'}],\n",
       " 'arxiv_primary_category': {'term': 'econ.GN',\n",
       "  'scheme': 'http://arxiv.org/schemas/atom'},\n",
       " 'tags': [{'term': 'econ.GN',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'label': None},\n",
       "  {'term': 'physics.soc-ph',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'label': None},\n",
       "  {'term': 'q-fin.EC',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom',\n",
       "   'label': None}]}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pap_merged[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f3d30604-5d55-4302-a798-678171692ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs: 16377\n",
      "eess: 6059\n",
      "physics: 11453\n",
      "math: 10147\n",
      "stat: 5680\n",
      "q-bio: 6302\n",
      "econ: 5347\n",
      "q-fin: 5613\n"
     ]
    }
   ],
   "source": [
    "for theme, pap in papers_classif.items():\n",
    "    print(f'{theme}: {sum(map(len, pap.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "13f319ae-4959-4c00-bba1-1f798b7eaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_classif, ids = classify_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e911f24-ac95-409d-afa2-af5ec007ada8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66978"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "385b6bb6-7c52-45d5-abd7-8d2ce387de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-fin.TR - 513\n",
      "q-fin.ST - 980\n",
      "q-fin.PM - 537\n",
      "q-fin.GN - 641\n",
      "q-fin.RM - 634\n",
      "q-fin.CP - 640\n",
      "q-fin.MF - 1048\n",
      "q-fin.PR - 418\n",
      "q-fin.EC - 202\n"
     ]
    }
   ],
   "source": [
    "for key, val in t['q-fin'].items():\n",
    "    print(f'{key} - {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "404e9929-1aec-4783-b01c-0c5a3ca131ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching arXiv for cat%3A%28cs.AI%20OR%20cs.AR%20OR%20cs.CC%20OR%20cs.CE%20OR%20cs.CG%20OR%20cs.CL%20OR%20cs.CR%20OR%20cs.CV%20OR%20cs.CY%20OR%20cs.DB%20OR%20cs.DC%20OR%20cs.DL%20OR%20cs.DM%20OR%20cs.DS%20OR%20cs.ET%20OR%20cs.FL%20OR%20cs.GL%20OR%20cs.GR%20OR%20cs.GT%20OR%20cs.HC%20OR%20cs.IR%20OR%20cs.IT%20OR%20cs.LG%20OR%20cs.LO%20OR%20cs.MA%20OR%20cs.MM%20OR%20cs.MS%20OR%20cs.NA%20OR%20cs.NA%20OR%20cs.NE%20OR%20cs.NI%20OR%20cs.OH%20OR%20cs.OS%20OR%20cs.PF%20OR%20cs.PL%20OR%20cs.RO%20OR%20cs.SC%20OR%20cs.SD%20OR%20cs.SE%20OR%20cs.SI%20OR%20cs.SY%20OR%20cs.SY%29\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "import time\n",
    "import feedparser\n",
    "\n",
    "# Base api query url;\n",
    "\n",
    "# Search parameters\n",
    "i = 0\n",
    "results_per_iteration = 50\n",
    "wait_time = 3\n",
    "papers = []\n",
    "year = \"\"  \n",
    "print('Searching arXiv for %s' % search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66628bbc-2ef9-42d5-99e0-363770cd8507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b98dbfc8-6535-49fc-b848-9327d5dd185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def get_chunk(theme, size, offset):\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    search_query = urllib.parse.quote(f\"({' OR '.join(f'cat:{subtheme}' for subtheme in set(themes_classif[theme]))})\")\n",
    "    query = f'search_query={search_query}&start={offset}&max_results={size}&sortBy=submittedDate&sortOrder=descending'\n",
    "    return urllib.request.urlopen(base_url+query).read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d5f29c5-e66a-452d-8770-c3de5ee2933f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924dfe0d602646c39e402bd0f65056cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs: got 600\n",
      "econ: got 7442\n",
      "eess: got 10000\n",
      "math: got 1800\n",
      "physics: got 600\n",
      "q-bio: got 1800\n",
      "q-fin: got 10000\n",
      "stat: got 10000\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# from time import sleep\n",
    "\n",
    "# size, offset = 10000, 200\n",
    "# papers = {}\n",
    "# pbar = tqdm(themes_classif)\n",
    "# for theme in pbar:\n",
    "#     pbar.set_description(f\"{theme}. Waiting...\")\n",
    "#     while not (entries := feedparser.parse(get_chunk(theme, size, offset))['entries']):\n",
    "#         pbar.set_description(f\"sleeping...\")\n",
    "#         sleep(10)\n",
    "#         pbar.set_description(f\"{theme}. Waiting...\")\n",
    "#     papers[theme] = entries\n",
    "#     pbar.set_description(f\"Success. sleeping...\")\n",
    "#     print(f\"{theme}: got {len(entries)}\")\n",
    "#     sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40561698-4085-44aa-b67d-6d86b580c973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(themes_classif['cs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05486f32-3d21-483d-9a9f-f094ed67f720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(cat:cs.AI OR cat:cs.AR OR cat:cs.CC OR cat:cs.CE OR cat:cs.CG OR cat:cs.CL OR cat:cs.CR OR cat:cs.CV OR cat:cs.CY OR cat:cs.DB OR cat:cs.DC OR cat:cs.DL OR cat:cs.DM OR cat:cs.DS OR cat:cs.ET OR cat:cs.FL OR cat:cs.GL OR cat:cs.GR OR cat:cs.GT OR cat:cs.HC OR cat:cs.IR OR cat:cs.IT OR cat:cs.LG OR cat:cs.LO OR cat:cs.MA OR cat:cs.MM OR cat:cs.MS OR cat:cs.NA OR cat:cs.NA OR cat:cs.NE OR cat:cs.NI OR cat:cs.OH OR cat:cs.OS OR cat:cs.PF OR cat:cs.PL OR cat:cs.RO OR cat:cs.SC OR cat:cs.SD OR cat:cs.SE OR cat:cs.SI OR cat:cs.SY OR cat:cs.SY)'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"({' OR '.join(f'cat:{subtheme}' for subtheme in set(themes_classif['cs']))})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ea50027-9806-40e3-a25c-6464d03c351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_size = len(papers['cs'])\n",
    "t = get_chunk(theme, 10, offset + cur_size)\n",
    "len(feedparser.parse(t)['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea99c28c-c475-4dc8-80ca-4586c5b8f4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1800, 7445, 10000, 6200, 1400, 10000, 10000, 10000]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, papers.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ee79f5d-dc69-4635-bef3-c2faf74495e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('loaded_arxiv.pickle', 'wb+') as f:\n",
    "    pickle.dump(papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "492a1be0-91bd-4e4d-a9f5-df6c541ad3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a8a1ae5a1543429cf56e8e5181ba0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics: got 500\n",
      "physics: got 500\n",
      "physics: got 500\n",
      "physics: got 500\n",
      "physics: got 500\n",
      "physics: got 500\n",
      "physics: got 300\n",
      "cs 10000\n",
      "econ 7445\n",
      "eess 10000\n",
      "math 10000\n",
      "physics 10000\n",
      "q-bio 10000\n",
      "q-fin 10000\n",
      "stat 10000\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(themes_classif)\n",
    "for theme in pbar:\n",
    "    cur_size = len(papers[theme])\n",
    "    if cur_size >= size or theme == 'econ':\n",
    "        continue\n",
    "    while (cur_size := len(papers[theme])) < size:\n",
    "        get_size = min(500, size - cur_size)\n",
    "        pbar.set_description(f\"{theme} - {get_size}. Waiting...\")\n",
    "        while not (entries := feedparser.parse(get_chunk(theme, get_size, offset + cur_size))['entries']):\n",
    "            pbar.set_description(f\"sleeping...\")\n",
    "            sleep(10)\n",
    "            pbar.set_description(f\"{theme}. Waiting...\")\n",
    "        papers[theme].extend(entries)\n",
    "        pbar.set_description(f\"Success. sleeping...\")\n",
    "        print(f\"{theme}: got {len(entries)}\")\n",
    "        sleep(10)\n",
    "for theme, pap in papers.items():\n",
    "    print(theme, len(pap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "45b4114d-1e0d-4236-bc13-9f093493416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs 10000\n",
      "econ 7445\n",
      "eess 10000\n",
      "math 10000\n",
      "physics 10000\n",
      "q-bio 10000\n",
      "q-fin 10000\n",
      "stat 10000\n"
     ]
    }
   ],
   "source": [
    "for theme, pap in papers.items():\n",
    "    print(theme, len(pap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b5abd49-7c0d-40b5-9675-58e88ba25d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2022, tm_mon=10, tm_mday=8, tm_hour=21, tm_min=54, tm_sec=36, tm_wday=5, tm_yday=281, tm_isdst=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['q-bio'][-1]['published_parsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "674aa201-76db-4233-be39-6f2e0c29a13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'stat.ME': 2228,\n",
       "         'stat.ML': 1505,\n",
       "         'stat.AP': 744,\n",
       "         'stat.OT': 36,\n",
       "         'stat.CO': 272})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([tag for paper in papers['stat'] if (tag := paper['arxiv_primary_category']['term']) in themes_classif['stat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2188ebba-4f0a-467d-b8fa-55cef7485c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "papers = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda86007-e41b-455f-bece-923adf4de4e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# while (year != \"2018\"): #stop requesting when papers date reach 2018\n",
    "#     print(\"Results %i - %i\" % (i,i+results_per_iteration))\n",
    "    \n",
    "#     \n",
    "\n",
    "#     # perform a GET request using the base_url and query\n",
    "#     response = \n",
    "\n",
    "#     # parse the response using feedparser\n",
    "#     feed = feedparser.parse(response)\n",
    "#     # Run through each entry, and print out information\n",
    "#     for entry in feed.entries:\n",
    "#         #print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "#         #print('Title:  %s' % entry.title)\n",
    "#         #feedparser v4.1 only grabs the first author\n",
    "#         #print('First Author:  %s' % entry.author)\n",
    "#         paper = {}\n",
    "#         paper[\"date\"] = entry.published\n",
    "#         year = paper[\"date\"][0:4]\n",
    "#         paper[\"title\"] = entry.title\n",
    "#         paper[\"first_author\"] = entry.author\n",
    "#         paper[\"summary\"] = entry.summary\n",
    "#         papers.append(paper)\n",
    "#     # Sleep a bit before calling the API again\n",
    "#     print('Bulk: %i' % 1)\n",
    "#     i += results_per_iteration\n",
    "#     time.sleep(wait_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ddfd6775-7692-4431-866d-049327567a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8c1be15d-d692-43fa-8412-2215e1a99d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs.CV',\n",
       " 'cs.CV',\n",
       " 'cs.CV',\n",
       " 'cs.HC',\n",
       " 'cs.CC',\n",
       " 'cs.IT',\n",
       " 'cs.LG',\n",
       " 'quant-ph',\n",
       " 'cs.CL',\n",
       " 'eess.SY',\n",
       " 'cs.LO',\n",
       " 'eess.SY',\n",
       " 'cs.LG',\n",
       " 'stat.ML',\n",
       " 'cs.LO',\n",
       " 'cs.CV',\n",
       " 'cs.CL',\n",
       " 'cs.HC',\n",
       " 'cs.RO',\n",
       " 'cs.CL',\n",
       " 'cs.RO',\n",
       " 'math.CO',\n",
       " 'cs.DC',\n",
       " 'physics.ins-det',\n",
       " 'cs.NI',\n",
       " 'eess.IV',\n",
       " 'cs.CL',\n",
       " 'cs.LO',\n",
       " 'cs.LG',\n",
       " 'cs.CL',\n",
       " 'cs.GR',\n",
       " 'cs.SY',\n",
       " 'cs.CL',\n",
       " 'math.NA',\n",
       " 'cs.LG',\n",
       " 'eess.AS',\n",
       " 'cs.SI',\n",
       " 'math.NA',\n",
       " 'cs.CL',\n",
       " 'cs.NI',\n",
       " 'cs.CL',\n",
       " 'cs.GR',\n",
       " 'math.PR',\n",
       " 'cs.LG',\n",
       " 'cs.SE',\n",
       " 'cs.LG',\n",
       " 'cs.CV',\n",
       " 'cs.CV',\n",
       " 'cs.GT',\n",
       " 'math.AG']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t['arxiv_primary_category']['term'] for t in feed['entries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ea13603-b47f-4814-8efb-d8217ebe2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "t = Counter([t['arxiv_primary_category']['term'] for t in feed['entries']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0df404d-a26e-4675-aeb1-6bb7e0bdb586",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://arxiv.org/abs/2305.04926v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04926v1',\n",
       "  'updated': '2023-05-08T17:59:58Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=59, tm_sec=58, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T17:59:58Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=59, tm_sec=58, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'RelPose++: Recovering 6D Poses from Sparse-view Observations',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'RelPose++: Recovering 6D Poses from Sparse-view Observations'},\n",
       "  'summary': 'We address the task of estimating 6D camera poses from sparse-view image sets\\n(2-8 images). This task is a vital pre-processing stage for nearly all\\ncontemporary (neural) reconstruction algorithms but remains challenging given\\nsparse views, especially for objects with visual symmetries and texture-less\\nsurfaces. We build on the recent RelPose framework which learns a network that\\ninfers distributions over relative rotations over image pairs. We extend this\\napproach in two key ways; first, we use attentional transformer layers to\\nprocess multiple images jointly, since additional views of an object may\\nresolve ambiguous symmetries in any given image pair (such as the handle of a\\nmug that becomes visible in a third view). Second, we augment this network to\\nalso report camera translations by defining an appropriate coordinate system\\nthat decouples the ambiguity in rotation estimation from translation\\nprediction. Our final system results in large improvements in 6D pose\\nprediction over prior art on both seen and unseen object categories and also\\nenables pose estimation and 3D reconstruction for in-the-wild objects.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'We address the task of estimating 6D camera poses from sparse-view image sets\\n(2-8 images). This task is a vital pre-processing stage for nearly all\\ncontemporary (neural) reconstruction algorithms but remains challenging given\\nsparse views, especially for objects with visual symmetries and texture-less\\nsurfaces. We build on the recent RelPose framework which learns a network that\\ninfers distributions over relative rotations over image pairs. We extend this\\napproach in two key ways; first, we use attentional transformer layers to\\nprocess multiple images jointly, since additional views of an object may\\nresolve ambiguous symmetries in any given image pair (such as the handle of a\\nmug that becomes visible in a third view). Second, we augment this network to\\nalso report camera translations by defining an appropriate coordinate system\\nthat decouples the ambiguity in rotation estimation from translation\\nprediction. Our final system results in large improvements in 6D pose\\nprediction over prior art on both seen and unseen object categories and also\\nenables pose estimation and 3D reconstruction for in-the-wild objects.'},\n",
       "  'authors': [{'name': 'Amy Lin'},\n",
       "   {'name': 'Jason Y. Zhang'},\n",
       "   {'name': 'Deva Ramanan'},\n",
       "   {'name': 'Shubham Tulsiani'}],\n",
       "  'author_detail': {'name': 'Shubham Tulsiani'},\n",
       "  'author': 'Shubham Tulsiani',\n",
       "  'arxiv_comment': 'Project webpage: https://amyxlase.github.io/relpose-plus-plus',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04926v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04926v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04925v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04925v1',\n",
       "  'updated': '2023-05-08T17:59:14Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=59, tm_sec=14, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T17:59:14Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=59, tm_sec=14, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR\\n  Point Clouds',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'PillarNeXt: Rethinking Network Designs for 3D Object Detection in LiDAR\\n  Point Clouds'},\n",
       "  'summary': 'In order to deal with the sparse and unstructured raw point clouds, LiDAR\\nbased 3D object detection research mostly focuses on designing dedicated local\\npoint aggregators for fine-grained geometrical modeling. In this paper, we\\nrevisit the local point aggregators from the perspective of allocating\\ncomputational resources. We find that the simplest pillar based models perform\\nsurprisingly well considering both accuracy and latency. Additionally, we show\\nthat minimal adaptions from the success of 2D object detection, such as\\nenlarging receptive field, significantly boost the performance. Extensive\\nexperiments reveal that our pillar based networks with modernized designs in\\nterms of architecture and training render the state-of-the-art performance on\\nthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our results\\nchallenge the common intuition that the detailed geometry modeling is essential\\nto achieve high performance for 3D object detection.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'In order to deal with the sparse and unstructured raw point clouds, LiDAR\\nbased 3D object detection research mostly focuses on designing dedicated local\\npoint aggregators for fine-grained geometrical modeling. In this paper, we\\nrevisit the local point aggregators from the perspective of allocating\\ncomputational resources. We find that the simplest pillar based models perform\\nsurprisingly well considering both accuracy and latency. Additionally, we show\\nthat minimal adaptions from the success of 2D object detection, such as\\nenlarging receptive field, significantly boost the performance. Extensive\\nexperiments reveal that our pillar based networks with modernized designs in\\nterms of architecture and training render the state-of-the-art performance on\\nthe two popular benchmarks: Waymo Open Dataset and nuScenes. Our results\\nchallenge the common intuition that the detailed geometry modeling is essential\\nto achieve high performance for 3D object detection.'},\n",
       "  'authors': [{'name': 'Jinyu Li'},\n",
       "   {'name': 'Chenxu Luo'},\n",
       "   {'name': 'Xiaodong Yang'}],\n",
       "  'author_detail': {'name': 'Xiaodong Yang'},\n",
       "  'author': 'Xiaodong Yang',\n",
       "  'arxiv_comment': 'CVPR 2023',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04925v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04925v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04923v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04923v1',\n",
       "  'updated': '2023-05-08T17:58:27Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=58, tm_sec=27, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T17:58:27Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=58, tm_sec=27, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'Learning to Evaluate the Artness of AI-generated Images',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Learning to Evaluate the Artness of AI-generated Images'},\n",
       "  'summary': 'Assessing the artness of AI-generated images continues to be a challenge\\nwithin the realm of image generation. Most existing metrics cannot be used to\\nperform instance-level and reference-free artness evaluation. This paper\\npresents ArtScore, a metric designed to evaluate the degree to which an image\\nresembles authentic artworks by artists (or conversely photographs), thereby\\noffering a novel approach to artness assessment. We first blend pre-trained\\nmodels for photo and artwork generation, resulting in a series of mixed models.\\nSubsequently, we utilize these mixed models to generate images exhibiting\\nvarying degrees of artness with pseudo-annotations. Each photorealistic image\\nhas a corresponding artistic counterpart and a series of interpolated images\\nthat range from realistic to artistic. This dataset is then employed to train a\\nneural network that learns to estimate quantized artness levels of arbitrary\\nimages. Extensive experiments reveal that the artness levels predicted by\\nArtScore align more closely with human artistic evaluation than existing\\nevaluation metrics, such as Gram loss and ArtFID.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Assessing the artness of AI-generated images continues to be a challenge\\nwithin the realm of image generation. Most existing metrics cannot be used to\\nperform instance-level and reference-free artness evaluation. This paper\\npresents ArtScore, a metric designed to evaluate the degree to which an image\\nresembles authentic artworks by artists (or conversely photographs), thereby\\noffering a novel approach to artness assessment. We first blend pre-trained\\nmodels for photo and artwork generation, resulting in a series of mixed models.\\nSubsequently, we utilize these mixed models to generate images exhibiting\\nvarying degrees of artness with pseudo-annotations. Each photorealistic image\\nhas a corresponding artistic counterpart and a series of interpolated images\\nthat range from realistic to artistic. This dataset is then employed to train a\\nneural network that learns to estimate quantized artness levels of arbitrary\\nimages. Extensive experiments reveal that the artness levels predicted by\\nArtScore align more closely with human artistic evaluation than existing\\nevaluation metrics, such as Gram loss and ArtFID.'},\n",
       "  'authors': [{'name': 'Junyu Chen'},\n",
       "   {'name': 'Jie An'},\n",
       "   {'name': 'Hanjia Lyu'},\n",
       "   {'name': 'Jiebo Luo'}],\n",
       "  'author_detail': {'name': 'Jiebo Luo'},\n",
       "  'author': 'Jiebo Luo',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04923v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04923v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.AI',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04868v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04868v1',\n",
       "  'updated': '2023-05-08T17:16:38Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=16, tm_sec=38, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T17:16:38Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=16, tm_sec=38, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign\\n  Language Understanding',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign\\n  Language Understanding'},\n",
       "  'summary': 'Hand gesture serves as a crucial role during the expression of sign language.\\nCurrent deep learning based methods for sign language understanding (SLU) are\\nprone to over-fitting due to insufficient sign data resource and suffer limited\\ninterpretability. In this paper, we propose the first self-supervised\\npre-trainable SignBERT+ framework with model-aware hand prior incorporated. In\\nour framework, the hand pose is regarded as a visual token, which is derived\\nfrom an off-the-shelf detector. Each visual token is embedded with gesture\\nstate and spatial-temporal position encoding. To take full advantage of current\\nsign data resource, we first perform self-supervised learning to model its\\nstatistics. To this end, we design multi-level masked modeling strategies\\n(joint, frame and clip) to mimic common failure detection cases. Jointly with\\nthese masked modeling strategies, we incorporate model-aware hand prior to\\nbetter capture hierarchical context over the sequence. After the pre-training,\\nwe carefully design simple yet effective prediction heads for downstream tasks.\\nTo validate the effectiveness of our framework, we perform extensive\\nexperiments on three main SLU tasks, involving isolated and continuous sign\\nlanguage recognition (SLR), and sign language translation (SLT). Experimental\\nresults demonstrate the effectiveness of our method, achieving new\\nstate-of-the-art performance with a notable gain.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Hand gesture serves as a crucial role during the expression of sign language.\\nCurrent deep learning based methods for sign language understanding (SLU) are\\nprone to over-fitting due to insufficient sign data resource and suffer limited\\ninterpretability. In this paper, we propose the first self-supervised\\npre-trainable SignBERT+ framework with model-aware hand prior incorporated. In\\nour framework, the hand pose is regarded as a visual token, which is derived\\nfrom an off-the-shelf detector. Each visual token is embedded with gesture\\nstate and spatial-temporal position encoding. To take full advantage of current\\nsign data resource, we first perform self-supervised learning to model its\\nstatistics. To this end, we design multi-level masked modeling strategies\\n(joint, frame and clip) to mimic common failure detection cases. Jointly with\\nthese masked modeling strategies, we incorporate model-aware hand prior to\\nbetter capture hierarchical context over the sequence. After the pre-training,\\nwe carefully design simple yet effective prediction heads for downstream tasks.\\nTo validate the effectiveness of our framework, we perform extensive\\nexperiments on three main SLU tasks, involving isolated and continuous sign\\nlanguage recognition (SLR), and sign language translation (SLT). Experimental\\nresults demonstrate the effectiveness of our method, achieving new\\nstate-of-the-art performance with a notable gain.'},\n",
       "  'authors': [{'name': 'Hezhen Hu'},\n",
       "   {'name': 'Weichao Zhao'},\n",
       "   {'name': 'Wengang Zhou'},\n",
       "   {'name': 'Houqiang Li'}],\n",
       "  'author_detail': {'name': 'Houqiang Li'},\n",
       "  'author': 'Houqiang Li',\n",
       "  'arxiv_doi': '10.1109/TPAMI.2023.3269220',\n",
       "  'links': [{'title': 'doi',\n",
       "    'href': 'http://dx.doi.org/10.1109/TPAMI.2023.3269220',\n",
       "    'rel': 'related',\n",
       "    'type': 'text/html'},\n",
       "   {'href': 'http://arxiv.org/abs/2305.04868v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04868v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_comment': 'Accepted to TPAMI. Project Page: https://signbert-zoo.github.io/',\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04844v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04844v1',\n",
       "  'updated': '2023-05-08T16:42:55Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=42, tm_sec=55, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T16:42:55Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=42, tm_sec=55, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'Compressed Video Quality Assessment for Super-Resolution: a Benchmark\\n  and a Quality Metric',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Compressed Video Quality Assessment for Super-Resolution: a Benchmark\\n  and a Quality Metric'},\n",
       "  'summary': \"We developed a super-resolution (SR) benchmark to analyze SR's capacity to\\nupscale compressed videos. Our dataset employed video codecs based on five\\ncompression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17\\nstate-ofthe-art SR models using our benchmark and evaluated their ability to\\npreserve scene context and their susceptibility to compression artifacts. To\\nget an accurate perceptual ranking of SR models, we conducted a crowd-sourced\\nside-by-side comparison of their outputs. The benchmark is publicly available\\nat\\nhttps://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html.\\nWe also analyzed benchmark results and developed an\\nobjective-quality-assessment metric based on the current bestperforming\\nobjective metrics. Our metric outperforms others, according to Spearman\\ncorrelation with subjective scores for compressed video upscaling. It is\\npublicly available at\\nhttps://github.com/EvgeneyBogatyrev/super-resolution-metric.\",\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': \"We developed a super-resolution (SR) benchmark to analyze SR's capacity to\\nupscale compressed videos. Our dataset employed video codecs based on five\\ncompression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 17\\nstate-ofthe-art SR models using our benchmark and evaluated their ability to\\npreserve scene context and their susceptibility to compression artifacts. To\\nget an accurate perceptual ranking of SR models, we conducted a crowd-sourced\\nside-by-side comparison of their outputs. The benchmark is publicly available\\nat\\nhttps://videoprocessing.ai/benchmarks/super-resolutionfor-video-compression.html.\\nWe also analyzed benchmark results and developed an\\nobjective-quality-assessment metric based on the current bestperforming\\nobjective metrics. Our metric outperforms others, according to Spearman\\ncorrelation with subjective scores for compressed video upscaling. It is\\npublicly available at\\nhttps://github.com/EvgeneyBogatyrev/super-resolution-metric.\"},\n",
       "  'authors': [{'name': 'Evgeney Bogatyrev'},\n",
       "   {'name': 'Ivan Molodetskikh'},\n",
       "   {'name': 'Dmitriy Vatolin'}],\n",
       "  'author_detail': {'name': 'Dmitriy Vatolin'},\n",
       "  'author': 'Dmitriy Vatolin',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04844v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04844v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'eess.IV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'eess.IV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04790v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04790v1',\n",
       "  'updated': '2023-05-08T15:45:42Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=45, tm_sec=42, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T15:45:42Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=45, tm_sec=42, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'MultiModal-GPT: A Vision and Language Model for Dialogue with Humans',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'MultiModal-GPT: A Vision and Language Model for Dialogue with Humans'},\n",
       "  'summary': 'We present a vision and language model named MultiModal-GPT to conduct\\nmulti-round dialogue with humans. MultiModal-GPT can follow various\\ninstructions from humans, such as generating a detailed caption, counting the\\nnumber of interested objects, and answering general questions from users.\\nMultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with\\nLow-rank Adapter (LoRA) added both in the cross-attention part and the\\nself-attention part of the language model. We first construct instruction\\ntemplates with vision and language data for multi-modality instruction tuning\\nto make the model understand and follow human instructions. We find the quality\\nof training data is vital for the dialogue performance, where few data\\ncontaining short answers can lead the model to respond shortly to any\\ninstructions. To further enhance the ability to chat with humans of the\\nMultiModal-GPT, we utilize language-only instruction-following data to train\\nthe MultiModal-GPT jointly. The joint training of language-only and\\nvisual-language instructions with the \\\\emph{same} instruction template\\neffectively improves dialogue performance. Various demos show the ability of\\ncontinuous dialogue of MultiModal-GPT with humans. Code and demo are at\\nhttps://github.com/open-mmlab/Multimodal-GPT',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'We present a vision and language model named MultiModal-GPT to conduct\\nmulti-round dialogue with humans. MultiModal-GPT can follow various\\ninstructions from humans, such as generating a detailed caption, counting the\\nnumber of interested objects, and answering general questions from users.\\nMultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with\\nLow-rank Adapter (LoRA) added both in the cross-attention part and the\\nself-attention part of the language model. We first construct instruction\\ntemplates with vision and language data for multi-modality instruction tuning\\nto make the model understand and follow human instructions. We find the quality\\nof training data is vital for the dialogue performance, where few data\\ncontaining short answers can lead the model to respond shortly to any\\ninstructions. To further enhance the ability to chat with humans of the\\nMultiModal-GPT, we utilize language-only instruction-following data to train\\nthe MultiModal-GPT jointly. The joint training of language-only and\\nvisual-language instructions with the \\\\emph{same} instruction template\\neffectively improves dialogue performance. Various demos show the ability of\\ncontinuous dialogue of MultiModal-GPT with humans. Code and demo are at\\nhttps://github.com/open-mmlab/Multimodal-GPT'},\n",
       "  'authors': [{'name': 'Tao Gong'},\n",
       "   {'name': 'Chengqi Lyu'},\n",
       "   {'name': 'Shilong Zhang'},\n",
       "   {'name': 'Yudong Wang'},\n",
       "   {'name': 'Miao Zheng'},\n",
       "   {'name': 'Qian Zhao'},\n",
       "   {'name': 'Kuikun Liu'},\n",
       "   {'name': 'Wenwei Zhang'},\n",
       "   {'name': 'Ping Luo'},\n",
       "   {'name': 'Kai Chen'}],\n",
       "  'author_detail': {'name': 'Kai Chen'},\n",
       "  'author': 'Kai Chen',\n",
       "  'arxiv_comment': '10 pages, 8 figures',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04790v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04790v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.CL',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04789v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04789v1',\n",
       "  'updated': '2023-05-08T15:43:00Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=43, tm_sec=0, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T15:43:00Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=43, tm_sec=0, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'AvatarReX: Real-time Expressive Full-body Avatars',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'AvatarReX: Real-time Expressive Full-body Avatars'},\n",
       "  'summary': 'We present AvatarReX, a new method for learning NeRF-based full-body avatars\\nfrom video data. The learnt avatar not only provides expressive control of the\\nbody, hands and the face together, but also supports real-time animation and\\nrendering. To this end, we propose a compositional avatar representation, where\\nthe body, hands and the face are separately modeled in a way that the\\nstructural prior from parametric mesh templates is properly utilized without\\ncompromising representation flexibility. Furthermore, we disentangle the\\ngeometry and appearance for each part. With these technical designs, we propose\\na dedicated deferred rendering pipeline, which can be executed in real-time\\nframerate to synthesize high-quality free-view images. The disentanglement of\\ngeometry and appearance also allows us to design a two-pass training strategy\\nthat combines volume rendering and surface rendering for network training. In\\nthis way, patch-level supervision can be applied to force the network to learn\\nsharp appearance details on the basis of geometry estimation. Overall, our\\nmethod enables automatic construction of expressive full-body avatars with\\nreal-time rendering capability, and can generate photo-realistic images with\\ndynamic details for novel body motions and facial expressions.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'We present AvatarReX, a new method for learning NeRF-based full-body avatars\\nfrom video data. The learnt avatar not only provides expressive control of the\\nbody, hands and the face together, but also supports real-time animation and\\nrendering. To this end, we propose a compositional avatar representation, where\\nthe body, hands and the face are separately modeled in a way that the\\nstructural prior from parametric mesh templates is properly utilized without\\ncompromising representation flexibility. Furthermore, we disentangle the\\ngeometry and appearance for each part. With these technical designs, we propose\\na dedicated deferred rendering pipeline, which can be executed in real-time\\nframerate to synthesize high-quality free-view images. The disentanglement of\\ngeometry and appearance also allows us to design a two-pass training strategy\\nthat combines volume rendering and surface rendering for network training. In\\nthis way, patch-level supervision can be applied to force the network to learn\\nsharp appearance details on the basis of geometry estimation. Overall, our\\nmethod enables automatic construction of expressive full-body avatars with\\nreal-time rendering capability, and can generate photo-realistic images with\\ndynamic details for novel body motions and facial expressions.'},\n",
       "  'authors': [{'name': 'Zerong Zheng'},\n",
       "   {'name': 'Xiaochen Zhao'},\n",
       "   {'name': 'Hongwen Zhang'},\n",
       "   {'name': 'Boning Liu'},\n",
       "   {'name': 'Yebin Liu'}],\n",
       "  'author_detail': {'name': 'Yebin Liu'},\n",
       "  'author': 'Yebin Liu',\n",
       "  'arxiv_comment': 'To appear in SIGGRAPH 2023 Journal Track. Project page at\\n  https://liuyebin.com/AvatarRex/',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04789v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04789v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.GR',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04769v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04769v1',\n",
       "  'updated': '2023-05-08T15:19:39Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=19, tm_sec=39, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T15:19:39Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=19, tm_sec=39, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning'},\n",
       "  'summary': 'The ability of deep neural networks to continually learn and adapt to a\\nsequence of tasks has remained challenging due to catastrophic forgetting of\\npreviously learned tasks. Humans, on the other hand, have a remarkable ability\\nto acquire, assimilate, and transfer knowledge across tasks throughout their\\nlifetime without catastrophic forgetting. The versatility of the brain can be\\nattributed to the rehearsal of abstract experiences through a complementary\\nlearning system. However, representation rehearsal in vision transformers lacks\\ndiversity, resulting in overfitting and consequently, performance drops\\nsignificantly compared to raw image rehearsal. Therefore, we propose BiRT, a\\nnovel representation rehearsal-based continual learning approach using vision\\ntransformers. Specifically, we introduce constructive noises at various stages\\nof the vision transformer and enforce consistency in predictions with respect\\nto an exponential moving average of the working model. Our method provides\\nconsistent performance gain over raw image and vanilla representation rehearsal\\non several challenging CL benchmarks, while being memory efficient and robust\\nto natural and adversarial corruptions.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'The ability of deep neural networks to continually learn and adapt to a\\nsequence of tasks has remained challenging due to catastrophic forgetting of\\npreviously learned tasks. Humans, on the other hand, have a remarkable ability\\nto acquire, assimilate, and transfer knowledge across tasks throughout their\\nlifetime without catastrophic forgetting. The versatility of the brain can be\\nattributed to the rehearsal of abstract experiences through a complementary\\nlearning system. However, representation rehearsal in vision transformers lacks\\ndiversity, resulting in overfitting and consequently, performance drops\\nsignificantly compared to raw image rehearsal. Therefore, we propose BiRT, a\\nnovel representation rehearsal-based continual learning approach using vision\\ntransformers. Specifically, we introduce constructive noises at various stages\\nof the vision transformer and enforce consistency in predictions with respect\\nto an exponential moving average of the working model. Our method provides\\nconsistent performance gain over raw image and vanilla representation rehearsal\\non several challenging CL benchmarks, while being memory efficient and robust\\nto natural and adversarial corruptions.'},\n",
       "  'authors': [{'name': 'Kishaan Jeeveswaran'},\n",
       "   {'name': 'Prashant Bhat'},\n",
       "   {'name': 'Bahram Zonooz'},\n",
       "   {'name': 'Elahe Arani'}],\n",
       "  'author_detail': {'name': 'Elahe Arani'},\n",
       "  'author': 'Elahe Arani',\n",
       "  'arxiv_comment': 'Accepted at 40th International Conference on Machine Learning (ICML\\n  2023)',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04769v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04769v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None},\n",
       "   {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
       "   {'term': 'cs.NE',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04766v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04766v1',\n",
       "  'updated': '2023-05-08T15:15:37Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=15, tm_sec=37, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T15:15:37Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=15, tm_sec=37, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation\\n  of Multichannel Images',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'OSTA: One-shot Task-adaptive Channel Selection for Semantic Segmentation\\n  of Multichannel Images'},\n",
       "  'summary': 'Semantic segmentation of multichannel images is a fundamental task for many\\napplications. Selecting an appropriate channel combination from the original\\nmultichannel image can improve the accuracy of semantic segmentation and reduce\\nthe cost of data storage, processing and future acquisition. Existing channel\\nselection methods typically use a reasonable selection procedure to determine a\\ndesirable channel combination, and then train a semantic segmentation network\\nusing that combination. In this study, the concept of pruning from a supernet\\nis used for the first time to integrate the selection of channel combination\\nand the training of a semantic segmentation network. Based on this concept, a\\nOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for the\\nsemantic segmentation of multichannel images. OSTA has three stages, namely the\\nsupernet training stage, the pruning stage and the fine-tuning stage. The\\noutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,\\nL8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiency\\nof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%\\n(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%\\n(mIoU), respectively). It even exceeded the highest accuracies of exhaustive\\ntests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)\\nand 70.27% (mIoU), respectively), where all possible channel combinations were\\ntested. All of this can be accomplished within a predictable and relatively\\nefficient timeframe, ranging from 101.71% to 298.1% times the time required to\\ntrain the segmentation network alone. In addition, there were interesting\\nfindings that were deemed valuable for several fields.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Semantic segmentation of multichannel images is a fundamental task for many\\napplications. Selecting an appropriate channel combination from the original\\nmultichannel image can improve the accuracy of semantic segmentation and reduce\\nthe cost of data storage, processing and future acquisition. Existing channel\\nselection methods typically use a reasonable selection procedure to determine a\\ndesirable channel combination, and then train a semantic segmentation network\\nusing that combination. In this study, the concept of pruning from a supernet\\nis used for the first time to integrate the selection of channel combination\\nand the training of a semantic segmentation network. Based on this concept, a\\nOne-Shot Task-Adaptive (OSTA) channel selection method is proposed for the\\nsemantic segmentation of multichannel images. OSTA has three stages, namely the\\nsupernet training stage, the pruning stage and the fine-tuning stage. The\\noutcomes of six groups of experiments (L7Irish3C, L7Irish2C, L8Biome3C,\\nL8Biome2C, RIT-18 and Semantic3D) demonstrated the effectiveness and efficiency\\nof OSTA. OSTA achieved the highest segmentation accuracies in all tests (62.49%\\n(mIoU), 75.40% (mIoU), 68.38% (mIoU), 87.63% (mIoU), 66.53% (mA) and 70.86%\\n(mIoU), respectively). It even exceeded the highest accuracies of exhaustive\\ntests (61.54% (mIoU), 74.91% (mIoU), 67.94% (mIoU), 87.32% (mIoU), 65.32% (mA)\\nand 70.27% (mIoU), respectively), where all possible channel combinations were\\ntested. All of this can be accomplished within a predictable and relatively\\nefficient timeframe, ranging from 101.71% to 298.1% times the time required to\\ntrain the segmentation network alone. In addition, there were interesting\\nfindings that were deemed valuable for several fields.'},\n",
       "  'authors': [{'name': 'Yuanzhi Cai'},\n",
       "   {'name': 'Jagannath Aryal'},\n",
       "   {'name': 'Yuan Fang'},\n",
       "   {'name': 'Hong Huang'},\n",
       "   {'name': 'Lei Fan'}],\n",
       "  'author_detail': {'name': 'Lei Fan'},\n",
       "  'author': 'Lei Fan',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04766v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04766v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]},\n",
       " {'id': 'http://arxiv.org/abs/2305.04763v1',\n",
       "  'guidislink': True,\n",
       "  'link': 'http://arxiv.org/abs/2305.04763v1',\n",
       "  'updated': '2023-05-08T15:11:28Z',\n",
       "  'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=11, tm_sec=28, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'published': '2023-05-08T15:11:28Z',\n",
       "  'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=15, tm_min=11, tm_sec=28, tm_wday=0, tm_yday=128, tm_isdst=0),\n",
       "  'title': 'Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief\\n  Propagation',\n",
       "  'title_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Large-scale and Efficient Texture Mapping Algorithm via Loopy Belief\\n  Propagation'},\n",
       "  'summary': 'Texture mapping as a fundamental task in 3D modeling has been well\\nestablished for well-acquired aerial assets under consistent illumination, yet\\nit remains a challenge when it is scaled to large datasets with images under\\nvarying views and illuminations. A well-performed texture mapping algorithm\\nmust be able to efficiently select views, fuse and map textures from these\\nviews to mesh models, at the same time, achieve consistent radiometry over the\\nentire model. Existing approaches achieve efficiency either by limiting the\\nnumber of images to one view per face, or simplifying global inferences to only\\nachieve local color consistency. In this paper, we break this tie by proposing\\na novel and efficient texture mapping framework that allows the use of multiple\\nviews of texture per face, at the same time to achieve global color\\nconsistency. The proposed method leverages a loopy belief propagation algorithm\\nto perform an efficient and global-level probabilistic inferences to rank\\ncandidate views per face, which enables face-level multi-view texture fusion\\nand blending. The texture fusion algorithm, being non-parametric, brings\\nanother advantage over typical parametric post color correction methods, due to\\nits improved robustness to non-linear illumination differences. The experiments\\non three different types of datasets (i.e. satellite dataset, unmanned-aerial\\nvehicle dataset and close-range dataset) show that the proposed method has\\nproduced visually pleasant and texturally consistent results in all scenarios,\\nwith an added advantage of consuming less running time as compared to the state\\nof the art methods, especially for large-scale dataset such as\\nsatellite-derived models.',\n",
       "  'summary_detail': {'type': 'text/plain',\n",
       "   'language': None,\n",
       "   'base': '',\n",
       "   'value': 'Texture mapping as a fundamental task in 3D modeling has been well\\nestablished for well-acquired aerial assets under consistent illumination, yet\\nit remains a challenge when it is scaled to large datasets with images under\\nvarying views and illuminations. A well-performed texture mapping algorithm\\nmust be able to efficiently select views, fuse and map textures from these\\nviews to mesh models, at the same time, achieve consistent radiometry over the\\nentire model. Existing approaches achieve efficiency either by limiting the\\nnumber of images to one view per face, or simplifying global inferences to only\\nachieve local color consistency. In this paper, we break this tie by proposing\\na novel and efficient texture mapping framework that allows the use of multiple\\nviews of texture per face, at the same time to achieve global color\\nconsistency. The proposed method leverages a loopy belief propagation algorithm\\nto perform an efficient and global-level probabilistic inferences to rank\\ncandidate views per face, which enables face-level multi-view texture fusion\\nand blending. The texture fusion algorithm, being non-parametric, brings\\nanother advantage over typical parametric post color correction methods, due to\\nits improved robustness to non-linear illumination differences. The experiments\\non three different types of datasets (i.e. satellite dataset, unmanned-aerial\\nvehicle dataset and close-range dataset) show that the proposed method has\\nproduced visually pleasant and texturally consistent results in all scenarios,\\nwith an added advantage of consuming less running time as compared to the state\\nof the art methods, especially for large-scale dataset such as\\nsatellite-derived models.'},\n",
       "  'authors': [{'name': 'Xiao ling'}, {'name': 'Rongjun Qin'}],\n",
       "  'author_detail': {'name': 'Rongjun Qin'},\n",
       "  'author': 'Rongjun Qin',\n",
       "  'arxiv_comment': '13 Figures',\n",
       "  'links': [{'href': 'http://arxiv.org/abs/2305.04763v1',\n",
       "    'rel': 'alternate',\n",
       "    'type': 'text/html'},\n",
       "   {'title': 'pdf',\n",
       "    'href': 'http://arxiv.org/pdf/2305.04763v1',\n",
       "    'rel': 'related',\n",
       "    'type': 'application/pdf'}],\n",
       "  'arxiv_primary_category': {'term': 'cs.CV',\n",
       "   'scheme': 'http://arxiv.org/schemas/atom'},\n",
       "  'tags': [{'term': 'cs.CV',\n",
       "    'scheme': 'http://arxiv.org/schemas/atom',\n",
       "    'label': None}]}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed['entries'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7c0e0-3148-40dc-96f2-6fcc178c01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # parse the response using feedparser\n",
    "    \n",
    "    # Run through each entry, and print out information\n",
    "    for entry in feed.entries:\n",
    "        #print('arxiv-id: %s' % entry.id.split('/abs/')[-1])\n",
    "        #print('Title:  %s' % entry.title)\n",
    "        #feedparser v4.1 only grabs the first author\n",
    "        #print('First Author:  %s' % entry.author)\n",
    "        paper = {}\n",
    "        paper[\"date\"] = entry.published\n",
    "        year = paper[\"date\"][0:4]\n",
    "        paper[\"title\"] = entry.title\n",
    "        paper[\"first_author\"] = entry.author\n",
    "        paper[\"summary\"] = entry.summary\n",
    "        papers.append(paper)\n",
    "    # Sleep a bit before calling the API again\n",
    "    print('Bulk: %i' % 1)\n",
    "    i += results_per_iteration\n",
    "    time.sleep(wait_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
