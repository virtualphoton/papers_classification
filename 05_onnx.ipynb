{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92416e08-7beb-4ff3-97ed-8a8f226198b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 11:21:40.008108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 11:21:40.510182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b4232b-a3fd-460e-9d37-2b8043810e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimum.onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8adf8704-89d8-45f6-984d-4e3d918db07e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast.SPECIAL_TOKENS_ATTRIBUTES\n",
       "DistilBertTokenizerFast.__annotations__\n",
       "DistilBertTokenizerFast.__base__\n",
       "DistilBertTokenizerFast.__bases__\n",
       "DistilBertTokenizerFast.__basicsize__\n",
       "DistilBertTokenizerFast.__call__\n",
       "DistilBertTokenizerFast.__class__\n",
       "DistilBertTokenizerFast.__delattr__\n",
       "DistilBertTokenizerFast.__dict__\n",
       "DistilBertTokenizerFast.__dictoffset__\n",
       "DistilBertTokenizerFast.__dir__\n",
       "DistilBertTokenizerFast.__doc__\n",
       "DistilBertTokenizerFast.__eq__\n",
       "DistilBertTokenizerFast.__flags__\n",
       "DistilBertTokenizerFast.__format__\n",
       "DistilBertTokenizerFast.__ge__\n",
       "DistilBertTokenizerFast.__getattribute__\n",
       "DistilBertTokenizerFast.__gt__\n",
       "DistilBertTokenizerFast.__hash__\n",
       "DistilBertTokenizerFast.__init__\n",
       "DistilBertTokenizerFast.__init_subclass__\n",
       "DistilBertTokenizerFast.__instancecheck__\n",
       "DistilBertTokenizerFast.__itemsize__\n",
       "DistilBertTokenizerFast.__le__\n",
       "DistilBertTokenizerFast.__len__\n",
       "DistilBertTokenizerFast.__lt__\n",
       "DistilBertTokenizerFast.__module__\n",
       "DistilBertTokenizerFast.__mro__\n",
       "DistilBertTokenizerFast.__name__\n",
       "DistilBertTokenizerFast.__ne__\n",
       "DistilBertTokenizerFast.__new__\n",
       "DistilBertTokenizerFast.__or__\n",
       "DistilBertTokenizerFast.__prepare__\n",
       "DistilBertTokenizerFast.__qualname__\n",
       "DistilBertTokenizerFast.__reduce__\n",
       "DistilBertTokenizerFast.__reduce_ex__\n",
       "DistilBertTokenizerFast.__repr__\n",
       "DistilBertTokenizerFast.__ror__\n",
       "DistilBertTokenizerFast.__setattr__\n",
       "DistilBertTokenizerFast.__sizeof__\n",
       "DistilBertTokenizerFast.__str__\n",
       "DistilBertTokenizerFast.__subclasscheck__\n",
       "DistilBertTokenizerFast.__subclasses__\n",
       "DistilBertTokenizerFast.__subclasshook__\n",
       "DistilBertTokenizerFast.__text_signature__\n",
       "DistilBertTokenizerFast.__weakref__\n",
       "DistilBertTokenizerFast.__weakrefoffset__\n",
       "DistilBertTokenizerFast.add_special_tokens\n",
       "DistilBertTokenizerFast.add_tokens\n",
       "DistilBertTokenizerFast.additional_special_tokens\n",
       "DistilBertTokenizerFast.additional_special_tokens_ids\n",
       "DistilBertTokenizerFast.all_special_ids\n",
       "DistilBertTokenizerFast.all_special_tokens\n",
       "DistilBertTokenizerFast.all_special_tokens_extended\n",
       "DistilBertTokenizerFast.as_target_tokenizer\n",
       "DistilBertTokenizerFast.backend_tokenizer\n",
       "DistilBertTokenizerFast.batch_decode\n",
       "DistilBertTokenizerFast.batch_encode_plus\n",
       "DistilBertTokenizerFast.bos_token\n",
       "DistilBertTokenizerFast.bos_token_id\n",
       "DistilBertTokenizerFast.build_inputs_with_special_tokens\n",
       "DistilBertTokenizerFast.can_save_slow_tokenizer\n",
       "DistilBertTokenizerFast.clean_up_tokenization\n",
       "DistilBertTokenizerFast.cls_token\n",
       "DistilBertTokenizerFast.cls_token_id\n",
       "DistilBertTokenizerFast.convert_ids_to_tokens\n",
       "DistilBertTokenizerFast.convert_tokens_to_ids\n",
       "DistilBertTokenizerFast.convert_tokens_to_string\n",
       "DistilBertTokenizerFast.create_token_type_ids_from_sequences\n",
       "DistilBertTokenizerFast.decode\n",
       "DistilBertTokenizerFast.decoder\n",
       "DistilBertTokenizerFast.encode\n",
       "DistilBertTokenizerFast.encode_plus\n",
       "DistilBertTokenizerFast.eos_token\n",
       "DistilBertTokenizerFast.eos_token_id\n",
       "DistilBertTokenizerFast.from_pretrained\n",
       "DistilBertTokenizerFast.get_added_vocab\n",
       "DistilBertTokenizerFast.get_special_tokens_mask\n",
       "DistilBertTokenizerFast.get_vocab\n",
       "DistilBertTokenizerFast.is_fast\n",
       "DistilBertTokenizerFast.mask_token\n",
       "DistilBertTokenizerFast.mask_token_id\n",
       "DistilBertTokenizerFast.max_len_sentences_pair\n",
       "DistilBertTokenizerFast.max_len_single_sentence\n",
       "DistilBertTokenizerFast.max_model_input_sizes\n",
       "DistilBertTokenizerFast.model_input_names\n",
       "DistilBertTokenizerFast.mro\n",
       "DistilBertTokenizerFast.num_special_tokens_to_add\n",
       "DistilBertTokenizerFast.pad\n",
       "DistilBertTokenizerFast.pad_token\n",
       "DistilBertTokenizerFast.pad_token_id\n",
       "DistilBertTokenizerFast.pad_token_type_id\n",
       "DistilBertTokenizerFast.padding_side\n",
       "DistilBertTokenizerFast.prepare_for_model\n",
       "DistilBertTokenizerFast.prepare_seq2seq_batch\n",
       "DistilBertTokenizerFast.pretrained_init_configuration\n",
       "DistilBertTokenizerFast.pretrained_vocab_files_map\n",
       "DistilBertTokenizerFast.push_to_hub\n",
       "DistilBertTokenizerFast.register_for_auto_class\n",
       "DistilBertTokenizerFast.sanitize_special_tokens\n",
       "DistilBertTokenizerFast.save_pretrained\n",
       "DistilBertTokenizerFast.save_vocabulary\n",
       "DistilBertTokenizerFast.sep_token\n",
       "DistilBertTokenizerFast.sep_token_id\n",
       "DistilBertTokenizerFast.set_truncation_and_padding\n",
       "DistilBertTokenizerFast.slow_tokenizer_class\n",
       "DistilBertTokenizerFast.special_tokens_map\n",
       "DistilBertTokenizerFast.special_tokens_map_extended\n",
       "DistilBertTokenizerFast.tokenize\n",
       "DistilBertTokenizerFast.train_new_from_iterator\n",
       "DistilBertTokenizerFast.truncate_sequences\n",
       "DistilBertTokenizerFast.truncation_side\n",
       "DistilBertTokenizerFast.unk_token\n",
       "DistilBertTokenizerFast.unk_token_id\n",
       "DistilBertTokenizerFast.vocab\n",
       "DistilBertTokenizerFast.vocab_files_names\n",
       "DistilBertTokenizerFast.vocab_size"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DistilBertTokenizerFast.*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92da2efe-7d13-43e3-bf27-ac24be615e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bea2804-773e-4cff-bc66-00c7e57ce1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "011083d6-18f6-4773-a43f-de1ce7685cbc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/photon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/photon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/photon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/photon/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "batch_text_or_text_pairs has to be a list or a tuple (got <class 'str'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRobust Model Selection with Application in Single-Cell Multiomics Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mDistilBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2807\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2808\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2813\u001b[0m )\n\u001b[0;32m-> 2815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:415\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_encode_plus\u001b[39m(\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    395\u001b[0m     batch_text_or_text_pairs: Union[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    412\u001b[0m     verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    413\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_text_or_text_pairs, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m--> 415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_text_or_text_pairs has to be a list or a tuple (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(batch_text_or_text_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    421\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    422\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    426\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_text_or_text_pairs has to be a list or a tuple (got <class 'str'>)"
     ]
    }
   ],
   "source": [
    "enc = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased').batch_encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics Data\",\n",
    "                                                                                     return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f8b5b-f6db-45ad-b6f0-336afe595146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52a600a1-a900-4588-a4f9-fce893301eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DistilBertForSequenceClassification(\n",
    "  (distilbert): DistilBertModel(\n",
    "    (embeddings): Embeddings(\n",
    "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "      (position_embeddings): Embedding(512, 768)\n",
    "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (transformer): Transformer(\n",
    "      (layer): ModuleList(\n",
    "        (0-5): 6 x TransformerBlock(\n",
    "          (attention): MultiHeadSelfAttention(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "          )\n",
    "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (ffn): FFN(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "            (activation): GELUActivation()\n",
    "          )\n",
    "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ca372e1-63b6-4a09-800a-6419c9000062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.0126, 0.0424, 0.0236, 0.0069, 0.4754, 0.0124, 0.3579]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**enc).logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037cccc2-56ad-47ba-ae40-da3c4b4b2e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1010, 1045, 1005, 1049, 3889, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased').encode_plus(\"Hi, I'm steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8504521-566a-4ac5-84d0-3c35a5ec1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers_interpret as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0803e8b8-a3f4-435e-84cb-8821e2ee797e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiLabelClassificationExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattribution_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lig'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcustom_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Explainer for independently explaining label attributions in a multi-label fashion\n",
       "for models of type `{MODEL_NAME}ForSequenceClassification` from the Transformers package.\n",
       "Every label is explained independently and the word attributions are a dictionary of labels\n",
       "mapping to the word attributions for that label. Even if the model itself is not multi-label\n",
       "by the resulting word attributions treat the labels as independent.\n",
       "\n",
       "Calculates attribution for `text` using the given model\n",
       "and tokenizer. Since this is a multi-label explainer, the attribution calculation time scales\n",
       "linearly with the number of labels.\n",
       "\n",
       "This explainer also allows for attributions with respect to a particlar embedding type.\n",
       "This can be selected by passing a `embedding_type`. The default value is `0` which\n",
       "is for word_embeddings, if `1` is passed then attributions are w.r.t to position_embeddings.\n",
       "If a model does not take position ids in its forward method (distilbert) a warning will\n",
       "occur and the default word_embeddings will be chosen instead.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Args:\n",
       "    model (PreTrainedModel): Pretrained huggingface Sequence Classification model.\n",
       "    tokenizer (PreTrainedTokenizer): Pretrained huggingface tokenizer\n",
       "    attribution_type (str, optional): The attribution method to calculate on. Defaults to \"lig\".\n",
       "    custom_labels (List[str], optional): Applies custom labels to label2id and id2label configs.\n",
       "                                         Labels must be same length as the base model configs' labels.\n",
       "                                         Labels and ids are applied index-wise. Defaults to None.\n",
       "\n",
       "Raises:\n",
       "    AttributionTypeNotSupportedError:\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers_interpret/explainers/text/multilabel_classification.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ti.MultiLabelClassificationExplainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "171b6c0a-533b-4629-8377-5002ae41be3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiLabelClassificationExplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membedding_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minternal_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Calculates attributions for `text` using the model\n",
       "and tokenizer given in the constructor. Attributions are calculated for\n",
       "every label output in the model.\n",
       "\n",
       "This explainer also allows for attributions with respect to a particlar embedding type.\n",
       "This can be selected by passing a `embedding_type`. The default value is `0` which\n",
       "is for word_embeddings, if `1` is passed then attributions are w.r.t to position_embeddings.\n",
       "If a model does not take position ids in its forward method (distilbert) a warning will\n",
       "occur and the default word_embeddings will be chosen instead.\n",
       "\n",
       "Args:\n",
       "    text (str): Text to provide attributions for.\n",
       "    embedding_type (int, optional): The embedding type word(0) or position(1) to calculate attributions for. Defaults to 0.\n",
       "    internal_batch_size (int, optional): Divides total #steps * #examples\n",
       "        data points into chunks of size at most internal_batch_size,\n",
       "        which are computed (forward / backward passes)\n",
       "        sequentially. If internal_batch_size is None, then all evaluations are\n",
       "        processed in one batch.\n",
       "    n_steps (int, optional): The number of steps used by the approximation\n",
       "        method. Default: 50.\n",
       "\n",
       "Returns:\n",
       "    dict: A dictionary of label to list of attributions.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers_interpret/explainers/text/multilabel_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ti.MultiLabelClassificationExplainer.__call__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d10cfcd6-5b11-4b2e-85a9-7c3216a757a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ti.BaseExplainer\n",
       "ti.ImageClassificationExplainer\n",
       "ti.MultiLabelClassificationExplainer\n",
       "ti.PairwiseSequenceClassificationExplainer\n",
       "ti.QuestionAnsweringExplainer\n",
       "ti.SequenceClassificationExplainer\n",
       "ti.TokenClassificationExplainer\n",
       "ti.ZeroShotClassificationExplainer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ti.*Explainer*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24842ce9-2879-4cff-9d80-59a93061e35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('robust', -0.021893395973169312),\n",
       " ('model', 0.2439568101880379),\n",
       " ('selection', 0.2676956189221858),\n",
       " ('with', 0.06079675413886155),\n",
       " ('application', -0.2181278733513814),\n",
       " ('in', -0.02050738327511616),\n",
       " ('single', 0.0043712554124574945),\n",
       " ('-', -0.021099933170305653),\n",
       " ('cell', 0.03409052761700392),\n",
       " ('multi', 0.011539601688422383),\n",
       " ('##omics', 0.7696297903952544),\n",
       " ('data', -0.4717611541577463),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_names = [\n",
    "    'Computer Science',\n",
    "    'Economics',\n",
    "    'Electrical Engineering and Systems Science',\n",
    "    'Mathematics',\n",
    "    'Physics',\n",
    "    'Quantitative Biology',\n",
    "    'Quantitative Finance',\n",
    "    'Statistics',\n",
    "]\n",
    "\n",
    "ti.SequenceClassificationExplainer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    custom_labels = tags_names\n",
    ")(\"Robust Model Selection with Application in Single-Cell Multiomics Data \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f71937c-3675-43cc-ad33-50d5d058049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=8,\n",
    "    output_attentions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63aff1e0-61af-4379-a7a1-77b101472a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15873,  2944,  4989,  2007,  4646,  1999,  2309,  1011,  3526,\n",
       "          4800, 25524,  2951,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics Data \", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13dc30f7-289c-4c4f-a7fb-6f790a9a0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.0126, 0.0424, 0.0236, 0.0069, 0.4754, 0.0124, 0.3579]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer.encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics Data \", return_tensors='pt')).logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dd8f87b-6ee8-42c7-b3b3-767ce143057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 13, 13])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344001ea-131d-4aa6-ac42-f7b3d50452e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=8,\n",
    "    output_attentions=True\n",
    ")\n",
    "model.load_state_dict(torch.load(\"./models/01_1_field.pt\"))\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac7f094-5596-4706-8d58-38e6b1fdd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "onnx_path = \"./models/onnx/model.onnx\"\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    str(onnx_path), \n",
    "    providers=['CPUExecutionProvider']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9aa14e5-12bf-4fef-a508-efa6f7e7d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "toks = tokenizer(\"Hi, I'm steve\",\n",
    "                 max_length=30, truncation=True, return_attention_mask=True,\n",
    "                 return_token_type_ids=False, return_tensors='np')\n",
    "feed = dict(\n",
    "    input_ids=np.array(toks[\"input_ids\"]).astype(\"int64\"),\n",
    "    attention_mask=np.array(toks[\"attention_mask\"]).astype(\"int64\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8812df5f-6abf-4c85-99ec-8de7995557cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.4442513, -1.7906857, -3.9432893, -2.7414489, -2.4700718,\n",
       "         -3.2154646, -2.965475 , -3.1665654]], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_session.run(None, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14019689-13e6-44a3-8dc4-111efe975788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./models/local-pt-checkpoint\")\n",
    "pt_model.save_pretrained(\"./models/local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b4dcd9-53c5-4b7a-8665-47150d1adbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:08:23.564021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 13:08:24.066522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884bdb1f591840269658f75ad57a025b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de532d3a333d4c7797fca6fee6502fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:08:48.604705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9406 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2023-05-11 13:08:49.187145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForQuestionAnswering\n",
    "\n",
    "distilbert = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "callable = tf.function(distilbert.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e9c968-5e8a-48a2-9ac2-70a3c19bb080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, 384], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, 384], tf.int32, name=\"attention_mask\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e93292-07c4-48e0-8199-809fffdfc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3726075300>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3726077cd0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3724522950>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372452d5d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372453c250>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372453ee90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:09:18.662922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:18.668577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:23.059263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:23.072756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 164). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilbert_cased_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilbert_cased_savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(distilbert, 'distilbert_cased_savedmodel', signatures=concrete_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca6c31-7fdf-48ac-afc5-a1a882614a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
