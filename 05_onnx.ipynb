{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92416e08-7beb-4ff3-97ed-8a8f226198b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 01:34:16.528353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 01:34:17.034181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b4232b-a3fd-460e-9d37-2b8043810e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimum.onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8adf8704-89d8-45f6-984d-4e3d918db07e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast.SPECIAL_TOKENS_ATTRIBUTES\n",
       "DistilBertTokenizerFast.__annotations__\n",
       "DistilBertTokenizerFast.__base__\n",
       "DistilBertTokenizerFast.__bases__\n",
       "DistilBertTokenizerFast.__basicsize__\n",
       "DistilBertTokenizerFast.__call__\n",
       "DistilBertTokenizerFast.__class__\n",
       "DistilBertTokenizerFast.__delattr__\n",
       "DistilBertTokenizerFast.__dict__\n",
       "DistilBertTokenizerFast.__dictoffset__\n",
       "DistilBertTokenizerFast.__dir__\n",
       "DistilBertTokenizerFast.__doc__\n",
       "DistilBertTokenizerFast.__eq__\n",
       "DistilBertTokenizerFast.__flags__\n",
       "DistilBertTokenizerFast.__format__\n",
       "DistilBertTokenizerFast.__ge__\n",
       "DistilBertTokenizerFast.__getattribute__\n",
       "DistilBertTokenizerFast.__gt__\n",
       "DistilBertTokenizerFast.__hash__\n",
       "DistilBertTokenizerFast.__init__\n",
       "DistilBertTokenizerFast.__init_subclass__\n",
       "DistilBertTokenizerFast.__instancecheck__\n",
       "DistilBertTokenizerFast.__itemsize__\n",
       "DistilBertTokenizerFast.__le__\n",
       "DistilBertTokenizerFast.__len__\n",
       "DistilBertTokenizerFast.__lt__\n",
       "DistilBertTokenizerFast.__module__\n",
       "DistilBertTokenizerFast.__mro__\n",
       "DistilBertTokenizerFast.__name__\n",
       "DistilBertTokenizerFast.__ne__\n",
       "DistilBertTokenizerFast.__new__\n",
       "DistilBertTokenizerFast.__or__\n",
       "DistilBertTokenizerFast.__prepare__\n",
       "DistilBertTokenizerFast.__qualname__\n",
       "DistilBertTokenizerFast.__reduce__\n",
       "DistilBertTokenizerFast.__reduce_ex__\n",
       "DistilBertTokenizerFast.__repr__\n",
       "DistilBertTokenizerFast.__ror__\n",
       "DistilBertTokenizerFast.__setattr__\n",
       "DistilBertTokenizerFast.__sizeof__\n",
       "DistilBertTokenizerFast.__str__\n",
       "DistilBertTokenizerFast.__subclasscheck__\n",
       "DistilBertTokenizerFast.__subclasses__\n",
       "DistilBertTokenizerFast.__subclasshook__\n",
       "DistilBertTokenizerFast.__text_signature__\n",
       "DistilBertTokenizerFast.__weakref__\n",
       "DistilBertTokenizerFast.__weakrefoffset__\n",
       "DistilBertTokenizerFast.add_special_tokens\n",
       "DistilBertTokenizerFast.add_tokens\n",
       "DistilBertTokenizerFast.additional_special_tokens\n",
       "DistilBertTokenizerFast.additional_special_tokens_ids\n",
       "DistilBertTokenizerFast.all_special_ids\n",
       "DistilBertTokenizerFast.all_special_tokens\n",
       "DistilBertTokenizerFast.all_special_tokens_extended\n",
       "DistilBertTokenizerFast.as_target_tokenizer\n",
       "DistilBertTokenizerFast.backend_tokenizer\n",
       "DistilBertTokenizerFast.batch_decode\n",
       "DistilBertTokenizerFast.batch_encode_plus\n",
       "DistilBertTokenizerFast.bos_token\n",
       "DistilBertTokenizerFast.bos_token_id\n",
       "DistilBertTokenizerFast.build_inputs_with_special_tokens\n",
       "DistilBertTokenizerFast.can_save_slow_tokenizer\n",
       "DistilBertTokenizerFast.clean_up_tokenization\n",
       "DistilBertTokenizerFast.cls_token\n",
       "DistilBertTokenizerFast.cls_token_id\n",
       "DistilBertTokenizerFast.convert_ids_to_tokens\n",
       "DistilBertTokenizerFast.convert_tokens_to_ids\n",
       "DistilBertTokenizerFast.convert_tokens_to_string\n",
       "DistilBertTokenizerFast.create_token_type_ids_from_sequences\n",
       "DistilBertTokenizerFast.decode\n",
       "DistilBertTokenizerFast.decoder\n",
       "DistilBertTokenizerFast.encode\n",
       "DistilBertTokenizerFast.encode_plus\n",
       "DistilBertTokenizerFast.eos_token\n",
       "DistilBertTokenizerFast.eos_token_id\n",
       "DistilBertTokenizerFast.from_pretrained\n",
       "DistilBertTokenizerFast.get_added_vocab\n",
       "DistilBertTokenizerFast.get_special_tokens_mask\n",
       "DistilBertTokenizerFast.get_vocab\n",
       "DistilBertTokenizerFast.is_fast\n",
       "DistilBertTokenizerFast.mask_token\n",
       "DistilBertTokenizerFast.mask_token_id\n",
       "DistilBertTokenizerFast.max_len_sentences_pair\n",
       "DistilBertTokenizerFast.max_len_single_sentence\n",
       "DistilBertTokenizerFast.max_model_input_sizes\n",
       "DistilBertTokenizerFast.model_input_names\n",
       "DistilBertTokenizerFast.mro\n",
       "DistilBertTokenizerFast.num_special_tokens_to_add\n",
       "DistilBertTokenizerFast.pad\n",
       "DistilBertTokenizerFast.pad_token\n",
       "DistilBertTokenizerFast.pad_token_id\n",
       "DistilBertTokenizerFast.pad_token_type_id\n",
       "DistilBertTokenizerFast.padding_side\n",
       "DistilBertTokenizerFast.prepare_for_model\n",
       "DistilBertTokenizerFast.prepare_seq2seq_batch\n",
       "DistilBertTokenizerFast.pretrained_init_configuration\n",
       "DistilBertTokenizerFast.pretrained_vocab_files_map\n",
       "DistilBertTokenizerFast.push_to_hub\n",
       "DistilBertTokenizerFast.register_for_auto_class\n",
       "DistilBertTokenizerFast.sanitize_special_tokens\n",
       "DistilBertTokenizerFast.save_pretrained\n",
       "DistilBertTokenizerFast.save_vocabulary\n",
       "DistilBertTokenizerFast.sep_token\n",
       "DistilBertTokenizerFast.sep_token_id\n",
       "DistilBertTokenizerFast.set_truncation_and_padding\n",
       "DistilBertTokenizerFast.slow_tokenizer_class\n",
       "DistilBertTokenizerFast.special_tokens_map\n",
       "DistilBertTokenizerFast.special_tokens_map_extended\n",
       "DistilBertTokenizerFast.tokenize\n",
       "DistilBertTokenizerFast.train_new_from_iterator\n",
       "DistilBertTokenizerFast.truncate_sequences\n",
       "DistilBertTokenizerFast.truncation_side\n",
       "DistilBertTokenizerFast.unk_token\n",
       "DistilBertTokenizerFast.unk_token_id\n",
       "DistilBertTokenizerFast.vocab\n",
       "DistilBertTokenizerFast.vocab_files_names\n",
       "DistilBertTokenizerFast.vocab_size"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DistilBertTokenizerFast.*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92da2efe-7d13-43e3-bf27-ac24be615e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bea2804-773e-4cff-bc66-00c7e57ce1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "011083d6-18f6-4773-a43f-de1ce7685cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = transformers.DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\\\n",
    "        .encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics\",\n",
    "                           return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f712c5f-36d9-4489-8c0a-8b9b54e94cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a30962d9-33ed-4ce4-9127-b3f0b624baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pre_classifier.weight = torch.nn.Parameter(torch.eye(768))\n",
    "t = torch.nn.Linear(768, 768)\n",
    "t.weights = torch.nn.Parameter(torch.eye(768))\n",
    "model.classifier = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d3c23ca-d610-43fc-b013-5ed381009797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15873,  2944,  4989,  2007,  4646,  1999,  2309,  1011,  3526,\n",
       "          4800, 25524,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59b3340a-9911-442f-85b2-057d566ee813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74da3747-d58c-4b3f-abad-ca2f86a3b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-5.7275e-01, -2.2921e-01, -2.6491e-01,  ..., -3.1681e-01,\n",
       "          -7.2820e-03,  3.0555e-01],\n",
       "         [-1.2105e-01,  2.9543e-04, -4.6220e-02,  ..., -1.9707e-01,\n",
       "           2.6295e-01, -1.9833e-01],\n",
       "         [-3.1855e-01, -2.8898e-01, -5.1896e-01,  ..., -6.5159e-01,\n",
       "          -7.7686e-02,  2.7950e-01],\n",
       "         ...,\n",
       "         [-9.9086e-02, -2.1533e-01,  1.3529e-01,  ..., -2.0891e-01,\n",
       "          -3.7595e-02, -1.3321e-01],\n",
       "         [-4.5175e-01, -1.2669e-01,  3.0670e-02,  ..., -4.1114e-01,\n",
       "          -1.1867e-01, -2.3275e-01],\n",
       "         [ 8.2036e-01, -8.9713e-02, -5.4693e-01,  ...,  2.8454e-01,\n",
       "          -8.3158e-01, -2.5317e-01]]], grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.distilbert(**enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18f8b5b-f6db-45ad-b6f0-336afe595146",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_2 = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ca372e1-63b6-4a09-800a-6419c9000062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.0126, 0.0424, 0.0236, 0.0069, 0.4754, 0.0124, 0.3579]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**enc).logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037cccc2-56ad-47ba-ae40-da3c4b4b2e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7632, 1010, 1045, 1005, 1049, 3889, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased').encode_plus(\"Hi, I'm steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8504521-566a-4ac5-84d0-3c35a5ec1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers_interpret as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24842ce9-2879-4cff-9d80-59a93061e35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('robust', -0.021893395973169312),\n",
       " ('model', 0.2439568101880379),\n",
       " ('selection', 0.2676956189221858),\n",
       " ('with', 0.06079675413886155),\n",
       " ('application', -0.2181278733513814),\n",
       " ('in', -0.02050738327511616),\n",
       " ('single', 0.0043712554124574945),\n",
       " ('-', -0.021099933170305653),\n",
       " ('cell', 0.03409052761700392),\n",
       " ('multi', 0.011539601688422383),\n",
       " ('##omics', 0.7696297903952544),\n",
       " ('data', -0.4717611541577463),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_names = [\n",
    "    'Computer Science',\n",
    "    'Economics',\n",
    "    'Electrical Engineering and Systems Science',\n",
    "    'Mathematics',\n",
    "    'Physics',\n",
    "    'Quantitative Biology',\n",
    "    'Quantitative Finance',\n",
    "    'Statistics',\n",
    "]\n",
    "\n",
    "ti.SequenceClassificationExplainer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    custom_labels = tags_names\n",
    ")(\"Robust Model Selection with Application in Single-Cell Multiomics Data \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f71937c-3675-43cc-ad33-50d5d058049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63aff1e0-61af-4379-a7a1-77b101472a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15873,  2944,  4989,  2007,  4646,  1999,  2309,  1011,  3526,\n",
       "          4800, 25524,  2951,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics Data \", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13dc30f7-289c-4c4f-a7fb-6f790a9a0965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0687, 0.0126, 0.0424, 0.0236, 0.0069, 0.4754, 0.0124, 0.3579]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**tokenizer.encode_plus(\"Robust Model Selection with Application in Single-Cell Multiomics Data \", return_tensors='pt')).logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344001ea-131d-4aa6-ac42-f7b3d50452e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=8,\n",
    "    output_attentions=True\n",
    ")\n",
    "model.load_state_dict(torch.load(\"./models/01_1_field.pt\"))\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac7f094-5596-4706-8d58-38e6b1fdd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "onnx_path = \"./models/onnx/model.onnx\"\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    str(onnx_path), \n",
    "    providers=['CPUExecutionProvider']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9aa14e5-12bf-4fef-a508-efa6f7e7d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "toks = tokenizer(\"Hi, I'm steve\",\n",
    "                 max_length=30, truncation=True, return_attention_mask=True,\n",
    "                 return_token_type_ids=False, return_tensors='np')\n",
    "feed = dict(\n",
    "    input_ids=np.array(toks[\"input_ids\"]).astype(\"int64\"),\n",
    "    attention_mask=np.array(toks[\"attention_mask\"]).astype(\"int64\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8812df5f-6abf-4c85-99ec-8de7995557cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.4442513, -1.7906857, -3.9432893, -2.7414489, -2.4700718,\n",
       "         -3.2154646, -2.965475 , -3.1665654]], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_session.run(None, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14019689-13e6-44a3-8dc4-111efe975788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokenizer.save_pretrained(\"./models/local-pt-checkpoint\")\n",
    "pt_model.save_pretrained(\"./models/local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b4dcd9-53c5-4b7a-8665-47150d1adbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:08:23.564021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-11 13:08:24.066522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884bdb1f591840269658f75ad57a025b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de532d3a333d4c7797fca6fee6502fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:08:48.604705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9406 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2023-05-11 13:08:49.187145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForQuestionAnswering\n",
    "\n",
    "distilbert = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
    "callable = tf.function(distilbert.call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e9c968-5e8a-48a2-9ac2-70a3c19bb080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concrete_function = callable.get_concrete_function([tf.TensorSpec([None, 384], tf.int32, name=\"input_ids\"), tf.TensorSpec([None, 384], tf.int32, name=\"attention_mask\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e93292-07c4-48e0-8199-809fffdfc0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3726075300>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3726077cd0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f3724522950>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372452d5d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372453c250>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.regularization.dropout.Dropout object at 0x7f372453ee90>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 13:09:18.662922: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:18.668577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:23.059263: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-05-11 13:09:23.072756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,?,768]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, transformer_layer_call_fn, transformer_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 164). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilbert_cased_savedmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: distilbert_cased_savedmodel/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(distilbert, 'distilbert_cased_savedmodel', signatures=concrete_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca6c31-7fdf-48ac-afc5-a1a882614a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
