{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65437a9-93c8-4f48-ab0e-c9b45803fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/cat_data.pickle', 'rb') as f:\n",
    "    sub_cat = pickle.load(f)[\"subcat\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8fcb53-599b-495c-bdf7-ff6112cd3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "cat_nums = defaultdict(list)\n",
    "with open('./data/cat_data.pickle', 'rb') as f:\n",
    "    cat_data = pickle.load(f)\n",
    "    cats, sub_cats = cat_data[\"cat\"], cat_data[\"subcat\"]\n",
    "cats = list(cats)\n",
    "for cat_num, sub_cat in enumerate(sub_cats):\n",
    "    cat_nums[cats.index(get_cat(sub_cat))].append(cat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82daa81e-6804-4350-9df5-4c8cc9e4e150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c6b4fc7-24df-4768-a4e0-d644c07ab418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack([torch.tensor(np.vstack([np.arange(89)]*5))[:, val].sum(axis=-1) for val in cat_nums.values()]).T.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a52c6d-746b-4f12-9a02-353b6b0a5e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 36, 37, 38, 40,\n",
       "        57, 58, 59, 60, 61, 62, 63, 83]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(89)[[list(cat_nums.values())[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea49166-f829-482c-a3d5-2c7a8c3a0dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 36, 37, 38, 40, 57, 58, 59, 60, 61, 62, 63, 83], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], [29, 30, 31], [32, 33, 34, 35], [39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56], [64, 65, 66, 67, 68, 69, 70, 71, 72, 73], [74, 75, 76, 77, 78, 79, 80, 81, 82], [84, 85, 86, 87, 88]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_nums.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1219d58a-3e44-464c-adcb-e2a53cc8acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/cat_data.pickle', 'rb') as f:\n",
    "    cat = pickle.load(f)[\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "782c1ebc-f1bf-4c89-88ec-7e2c670232de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_to_num = dict(zip(cat, range(len(sub_cat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816dfe08-4d74-442d-9e0e-0c0aad80f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_sub_cat = dict(zip(range(len(sub_cat)), sub_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8289f3e4-147a-419f-94ad-7d8b8c9b1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics = {'quant-ph', 'physics', 'nucl-th', 'nucl-ex', 'nlin', \n",
    "           'math-ph', 'hep-th', 'hep-ph', 'hep-lat', 'hep-ex', \n",
    "           'gr-qc', 'cond-mat', 'astro-ph', 'adap-org', 'physics'}\n",
    "\n",
    "tags_names = {\n",
    "    'cs': 'Computer Science',\n",
    "    'econ': 'Economics',\n",
    "    'eess': 'Electrical Engineering and Systems Science',\n",
    "    'math': 'Mathematics',\n",
    "    'physics': 'Physics',\n",
    "    'q-bio': 'Quantitative Biology',\n",
    "    'q-fin': 'Quantitative Finance',\n",
    "    'stat': 'Statistics',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a2eb0d1-74ec-4509-b1d6-1d8b9710f5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Accuracy classification score.\n",
       "\n",
       "In multilabel classification, this function computes subset accuracy:\n",
       "the set of labels predicted for a sample must *exactly* match the\n",
       "corresponding set of labels in y_true.\n",
       "\n",
       "Read more in the :ref:`User Guide <accuracy_score>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) labels.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Predicted labels, as returned by a classifier.\n",
       "\n",
       "normalize : bool, default=True\n",
       "    If ``False``, return the number of correctly classified samples.\n",
       "    Otherwise, return the fraction of correctly classified samples.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "score : float\n",
       "    If ``normalize == True``, return the fraction of correctly\n",
       "    classified samples (float), else returns the number of correctly\n",
       "    classified samples (int).\n",
       "\n",
       "    The best performance is 1 with ``normalize == True`` and the number\n",
       "    of samples with ``normalize == False``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
       "    imbalanced datasets.\n",
       "jaccard_score : Compute the Jaccard similarity coefficient score.\n",
       "hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
       "    two sets of samples.\n",
       "zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
       "    function will return the percentage of imperfectly predicted subsets.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In binary classification, this function is equal to the `jaccard_score`\n",
       "function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import accuracy_score\n",
       ">>> y_pred = [0, 2, 1, 3]\n",
       ">>> y_true = [0, 1, 2, 3]\n",
       ">>> accuracy_score(y_true, y_pred)\n",
       "0.5\n",
       ">>> accuracy_score(y_true, y_pred, normalize=False)\n",
       "2\n",
       "\n",
       "In the multilabel case with binary label indicators:\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
       "0.5\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/ML2/lib/python3.10/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f49c85e-0bdc-4816-a121-a8edcf0a1399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, DistilBertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import time, datetime, random, re\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from operator import itemgetter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "SEED = 15\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.amp.autocast(enabled=True)\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8dfc988-93db-4e64-af65-7d813a650a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "with open('./data/cat_data.pickle', 'rb') as f:\n",
    "    SUB_CATEGORIES_NUM = len(pickle.load(f)[\"subcat\"])\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, path=None, n_labels=SUB_CATEGORIES_NUM):\n",
    "        super().__init__()\n",
    "        self.transformer = None\n",
    "        if path is None:\n",
    "            self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(in_features=768, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Dropout(.2),\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Dropout(.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            torch.nn.BatchNorm1d(num_features=1024),\n",
    "            nn.Dropout(.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=n_labels),\n",
    "        )\n",
    "        \n",
    "        if path is not None:\n",
    "            self.load(path)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        X = self.transformer(input_ids=input_ids, attention_mask=attention_mask)[0][:, 0]\n",
    "        return self.clf(X)\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            path.mkdir()\n",
    "        for i, layer in enumerate([self.clf]):\n",
    "            torch.save(layer.state_dict(), path / f\"layer_{i}.pt\")\n",
    "        self.transformer.save_pretrained(path / \"transformer-chkp\")\n",
    "        \n",
    "    def load(self, path):\n",
    "        path = Path(path)\n",
    "        self.transformer = DistilBertModel.from_pretrained(\n",
    "            path / \"transformer-chkp\",\n",
    "            local_files_only=True\n",
    "        )\n",
    "        for i, layer in enumerate([self.clf]):\n",
    "            layer.load_state_dict(torch.load(path / f\"layer_{i}.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd144feb-7e50-41d4-8ba1-ce824702b5a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (clf): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (10): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): ReLU()\n",
       "    (13): Linear(in_features=1024, out_features=89, bias=True)\n",
       "  )\n",
       "  (transformer): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = Path(\"./models/07.1\")\n",
    "model = MyModel(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a43b33fd-8116-4916-a8d9-1e6f7f829273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[101, 102, 102]]), 'attention_mask': tensor([[1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "    title + tokenizer.sep_token + summary,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63fa6142-08b3-43de-aa7e-6c53897f06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"\"\n",
    "summary = \"\"\n",
    "MAX_LENGTH = 290\n",
    "tokens = tokenizer.encode_plus(\n",
    "    title + tokenizer.sep_token + summary,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3001046-b402-4218-96c8-ddb8156c1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_sum = defaultdict(float)\n",
    "for i, prob in enumerate(proba):\n",
    "    prob_sum[get_cat(num_to_sub_cat[i])] += prob.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "254ebd68-0392-40dd-8d0d-8e55ff15d4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'physics': 0.12544300840818323,\n",
       "             'cs': 0.11830023513175547,\n",
       "             'econ': 0.3506761244498193,\n",
       "             'eess': 0.013924322323873639,\n",
       "             'math': 0.10891761467792094,\n",
       "             'q-bio': 0.06461559410672635,\n",
       "             'q-fin': 0.1728671460878104,\n",
       "             'stat': 0.04525592748541385})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "451dcbb6-12d0-467a-a5be-50b38d670844",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs': 0.0014475085772573948,\n",
       " 'econ': 0.0017359015764668584,\n",
       " 'eess': 0.005249212961643934,\n",
       " 'math': 0.009026859886944294,\n",
       " 'physics': 0.0018133745761588216,\n",
       " 'q-bio': 0.018986567854881287,\n",
       " 'q-fin': 0.0014966991730034351,\n",
       " 'stat': 0.0022101285867393017}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "proba = logits.softmax(dim=-1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193d6a61-db35-42af-bb59-c550657d24bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cat(tag):\n",
    "    # returns tag category as number / -1 if unkown\n",
    "    tag = tag.split('.')[0]\n",
    "    if tag in physics:\n",
    "        return \"physics\"\n",
    "    return tag\n",
    "get_cat(\"physics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28dddcfe-87d3-4cde-94b5-4703eaaa0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/encoded.pickle', 'rb') as f:\n",
    "    pickle_obj = pickle.load(f)\n",
    "_, val_seq = pickle_obj['train'], pickle_obj['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20f94d96-be5d-4543-840c-6f61c5edc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq, epoch_size=None):\n",
    "        super().__init__()\n",
    "        self.X, self.y = seq\n",
    "        self.epoch_size = epoch_size or len(self.X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.epoch_size:\n",
    "            raise IndexError\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def reshuffle(self):\n",
    "        zipped = list(zip(self.X, self.y))\n",
    "        shuffle(zipped)\n",
    "        self.X, self.y = zip(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48a0e2af-49ec-4ea1-bdc0-173885e426fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = MyDataset(val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8be49c7-82a4-429e-bc77-307060942f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(logits_on_gpu):\n",
    "    return logits_on_gpu.argmax(axis=1).flatten().detach().numpy()\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    acc = 0\n",
    "    \n",
    "    dataloader.reshuffle()\n",
    "    pbar = tqdm(dataloader)\n",
    "    y_pred = []\n",
    "    y_true_list = []\n",
    "    for X_, y_true in pbar:\n",
    "        y_true_cpu = y_true\n",
    "        y_true = y_true\n",
    "                                        \n",
    "        with torch.no_grad():\n",
    "            X = X_[-1]\n",
    "            X = [x for x in X]\n",
    "            logits = model(*X)\n",
    "            logits = model(tokens[\"input_ids\"], tokens[\"attention_mask\"])\n",
    "            proba_ = logits.softmax(dim=-1).flatten()\n",
    "            \n",
    "            for proba_ in proba_:\n",
    "                prob_sum = defaultdict(float)\n",
    "                for i, prob in enumerate(proba):\n",
    "                    prob_sum[get_cat(num_to_sub_cat[i])] += prob.item()\n",
    "                y_pred.append(max(prob_sum, key=prob_sum.get))\n",
    "            y_true_list.append(\n",
    "                np.array([cat_to_num[get_cat(num_to_sub_cat[t])] for t in get_prediction(y_true)]))\n",
    "    return accuracy_score(np.array(y_pred, y_true_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74655590-04d7-4ba1-92d1-e426180d014d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b0435926244afca3f7178f613f468f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166132/3645955178.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_166132/2089308232.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mproba_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_166132/3883456424.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \"\"\"\n\u001b[1;32m   1500\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_state_dict_post_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML2/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validate(model, val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8dbc0-1364-4700-9219-3b6877bc6b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
